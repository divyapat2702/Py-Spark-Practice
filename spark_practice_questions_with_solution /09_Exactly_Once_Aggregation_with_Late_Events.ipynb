{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc489d63-933f-4d89-a13a-cbe3165d78a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a89e60-dda4-4839-8383-9551e07b0086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üîÅ Exactly-Once Aggregation with Late Events\n",
    "\n",
    "This notebook demonstrates how to build a **near exactly-once**\n",
    "real-time aggregation pipeline using **Spark Structured Streaming**.\n",
    "\n",
    "The pipeline handles:\n",
    "- late-arriving events (up to 2 hours)\n",
    "- streaming replays after failures\n",
    "- prevention of double counting using idempotent writes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1135d3cb-3383-49c0-be03-fb4066c927ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `transactions_stream_large.csv`  \n",
    "\n",
    "### Columns:\n",
    "- `event_time`\n",
    "- `processing_time`\n",
    "- `txn_id`\n",
    "- `customer_id`\n",
    "- `amount`\n",
    "- `status`\n",
    "\n",
    "> ‚ö†Ô∏è In production, this data would arrive from **Kafka**.  \n",
    "> For learning purposes, we simulate streaming using files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab12c9ed-7dd1-4583-8d0a-5a4bed87933d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are building a **real-time transaction monitoring system**.\n",
    "\n",
    "Challenges:\n",
    "- Events can arrive **late (up to 2 hours)**\n",
    "- Failures may cause Spark to **replay micro-batches**\n",
    "- You must **avoid double counting**\n",
    "\n",
    "The business requires:\n",
    "- accurate aggregates\n",
    "- tolerance to late data\n",
    "- fault-tolerant recovery without restarting the job\n",
    "\n",
    "Your task is to design a streaming job that provides\n",
    "**near exactly-once aggregates**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "1. Read transaction events as a stream\n",
    "2. Use **event-time processing**\n",
    "3. Allow late data (up to 2 hours) using watermarks\n",
    "4. Aggregate transactions per hour and per customer\n",
    "5. Write results in a way that **prevents double counting**\n",
    "6. Ensure the job can safely recover from failures\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Events may arrive **out of order**\n",
    "- Late events can arrive up to **2 hours**\n",
    "- Spark Serverless compute is used\n",
    "- Unity Catalog storage is available\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Hourly aggregated transaction amounts per customer\n",
    "- Aggregates must be:\n",
    "  - fault-tolerant\n",
    "  - idempotent\n",
    "  - safe against reprocessing\n",
    "\n",
    "### Expected Columns\n",
    "\n",
    "| window.start | window.end | customer_id | total_amount |\n",
    "|--------------|------------|-------------|--------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Spark provides **exactly-once processing of source offsets**\n",
    "- Aggregations are **near exactly-once**\n",
    "- Idempotent sinks prevent double counting\n",
    "- Watermarks define how late data is handled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0714b192-1d74-4648-880f-fd19edbd61d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read transactions as a streaming DataFrame\n",
    "2. Parse timestamps and filter valid events\n",
    "3. Use **event-time watermarks** (2 hours)\n",
    "4. Perform windowed aggregations\n",
    "5. Write aggregates to a **transactional staging table**\n",
    "6. Use **MERGE-based upserts** into the final table\n",
    "\n",
    "Spark handles:\n",
    "- offset tracking\n",
    "- state management\n",
    "- fault recovery via checkpoints\n",
    "\n",
    "Exactly-once behavior is achieved by combining:\n",
    "- checkpointed offsets\n",
    "- deterministic aggregation keys\n",
    "- idempotent MERGE writes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff926525-6d44-4db6-b931-07fec06c747c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2949e0-f792-40bc-a421-eb051de68b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è 1: Read Transactions as Batch (Source Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac9cf7c-0800-44b7-a9ad-073b05174dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_txn_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    "         .withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    "         .withColumn(\"processing_time\", F.to_timestamp(\"processing_time\"))\n",
    ")\n",
    "\n",
    "display(batch_txn_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "397ad41b-b3c9-4f84-a5c5-dcdfaf089821",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÅ 2: Convert Batch Data into Streaming Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f19f6e6-738c-4f18-a2f9-04cd534aee33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    batch_txn_df\n",
    "        .repartition(6)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(\"your_data\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edf2ab12-03f7-4e5e-bb5a-d157776f4f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è 3: Read Transactions as a Stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04822398-ad04-4363-992d-3e183e21a7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = \"\"\"\n",
    "event_time STRING,\n",
    "processing_time STRING,\n",
    "txn_id STRING,\n",
    "customer_id STRING,\n",
    "amount DOUBLE,\n",
    "status STRING\n",
    "\"\"\"\n",
    "\n",
    "transactions = (\n",
    "    spark.readStream\n",
    "         .schema(schema)\n",
    "         .option(\"header\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    "         .withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    "         .withColumn(\"processing_time\", F.to_timestamp(\"processing_time\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9e8fd82-7355-4f1a-8400-fac5bfecd415",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ 4: Filter Successful Transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff45b1a5-0a58-49dd-9e4b-dfb43d3e8237",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events = transactions.filter(F.col(\"status\") == \"SUCCESS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72f56999-8f63-4357-afbe-9167366f121a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚è±Ô∏è 5: Watermark + Windowed Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9ca000-d2dc-4aba-8141-e249cf99eba5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg = (\n",
    "    events\n",
    "        .withWatermark(\"event_time\", \"2 hours\")\n",
    "        .groupBy(\n",
    "            F.window(\"event_time\", \"1 hour\"),\n",
    "            F.col(\"customer_id\")\n",
    "        )\n",
    "        .agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfa62d1d-6269-48cf-b3bd-2590822af3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üì§ 6: Idempotent MERGE into Final Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93046ea4-b0dd-4f08-ac42-a71de4f83f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "final_path = \"your_directory\"\n",
    "\n",
    "def upsert_to_delta(batch_df, batch_id):\n",
    "    if not DeltaTable.isDeltaTable(spark, final_path):\n",
    "        batch_df.write.format(\"delta\").mode(\"overwrite\").save(final_path)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forPath(spark, final_path)\n",
    "        delta_tbl.alias(\"tgt\").merge(\n",
    "            batch_df.alias(\"src\"),\n",
    "            \"\"\"\n",
    "            tgt.customer_id = src.customer_id\n",
    "            AND tgt.window.start = src.window.start\n",
    "            AND tgt.window.end = src.window.end\n",
    "            \"\"\"\n",
    "        ).whenMatchedUpdateAll() \\\n",
    "         .whenNotMatchedInsertAll() \\\n",
    "         .execute()\n",
    "\n",
    "query = (\n",
    "    agg\n",
    "        .writeStream\n",
    "        .foreachBatch(upsert_to_delta)\n",
    "        .option(\n",
    "            \"checkpointLocation\",\n",
    "            \"your_directory\"\n",
    "        )\n",
    "        .trigger(availableNow=True)\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbbefbbb-f6ee-49cd-a16b-05c915c30057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Why This Achieves Near Exactly-Once\n",
    "\n",
    "- Source offsets are checkpointed\n",
    "- Aggregation keys are deterministic\n",
    "- MERGE is idempotent\n",
    "- Replayed micro-batches update existing rows\n",
    "- No duplicate aggregates are created\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdb10e0a-ea6c-4805-aa0a-8f8bbe806002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Watermarks handle late data safely\n",
    "- Delta MERGE ensures idempotent writes\n",
    "- Checkpoints provide fault tolerance\n",
    "- This pattern is production-grade and scalable\n",
    "\n",
    "This notebook demonstrates a **real-world exactly-once‚Äìstyle\n",
    "streaming aggregation design**.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09_Exactly_Once_Aggregation_with_Late_Events",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
