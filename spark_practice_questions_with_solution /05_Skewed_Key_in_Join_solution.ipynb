{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cbe260-efa7-44ad-8e9d-e811e3aead9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1682385c-e2f2-4ee2-9296-3e4a1b9f47a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚öñÔ∏è Handling Skewed Keys in Spark Joins\n",
    "\n",
    "This notebook demonstrates how **data skew** can severely impact Spark join performance\n",
    "and how to **detect and handle skewed keys** using proven Spark techniques.\n",
    "\n",
    "We focus on a common real-world issue where **a few keys dominate the data distribution**\n",
    "and cause one or more tasks to run significantly slower than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5a06ba-99d2-4741-aec3-ed63b7868326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "### Dataset A (Large & Skewed)\n",
    "**Dataset Name:** `transactions_a_skewed_large.csv`\n",
    "\n",
    "### Dataset B (Large & Skewed)\n",
    "**Dataset Name:** `transactions_b_skewed_large.csv`\n",
    "\n",
    "> ‚ö†Ô∏è These datasets simulate a real-world scenario where  \n",
    "a small number of `customer_id`s have **millions of records**,  \n",
    "while most customers have only a few.\n",
    "\n",
    "Both datasets are assumed to be available in **your catalog / database storage**.\n",
    "\n",
    "### Example Columns:\n",
    "- `transaction_id`\n",
    "- `customer_id`\n",
    "- `transaction_date`\n",
    "- `amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd5d4e5-9cbc-423e-a0ed-c57d4fbebc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are joining **two large transactional DataFrames** on `customer_id`.\n",
    "\n",
    "During execution, you notice that:\n",
    "- One or two tasks take **much longer** than others\n",
    "- Most tasks finish quickly, but a few keep running\n",
    "- Overall job time is dominated by a **single slow task**\n",
    "\n",
    "This usually indicates **data skew**, where a small number of keys\n",
    "(e.g., certain customers) have a **disproportionately large number of records**.\n",
    "\n",
    "Your goal is to:\n",
    "- **Detect** the skew\n",
    "- **Understand** why it happens\n",
    "- **Fix** the skew so the join runs efficiently\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps using Spark:\n",
    "\n",
    "1. **Read** both skewed transaction datasets.\n",
    "2. **Detect skewed keys** by analyzing record counts per `customer_id`.\n",
    "3. Confirm skew symptoms using **Spark UI**.\n",
    "4. **Handle skew** using key salting.\n",
    "5. (Optional) Enable **Adaptive Query Execution (AQE)** for automatic skew handling.\n",
    "6. Perform the join efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Both datasets are large and distributed across the cluster.\n",
    "- A small number of `customer_id`s are extremely frequent (hot keys).\n",
    "- Spark join performance is impacted by uneven partition sizes.\n",
    "- Spark Serverless or classic clusters may be used.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- **Joined DataFrame** with balanced execution\n",
    "- Reduced task skew and improved join performance\n",
    "\n",
    "### **Join Key**\n",
    "- `customer_id`\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes \n",
    "\n",
    "- Spark distributes work **by key** during joins.\n",
    "- If one key has far more records, **one task gets overloaded**.\n",
    "- Skew causes:\n",
    "  - slow tasks\n",
    "  - poor CPU utilization\n",
    "  - long job runtimes\n",
    "- Detecting skew early is critical for scalable pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a34b96-0871-4c38-a88c-fb58cf7e601b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read both large transaction datasets into Spark DataFrames.\n",
    "2. Detect skew by grouping on `customer_id` and identifying unusually high counts.\n",
    "3. Validate skew by observing slow or heavy tasks in the Spark UI.\n",
    "4. Identify **hot keys** (customers with extremely high record counts).\n",
    "5. Apply **salting** to the hot keys to spread their records across multiple partitions.\n",
    "6. Join the salted DataFrames on both `customer_id` and `salt`.\n",
    "7. Optionally enable **Adaptive Query Execution (AQE)** to let Spark handle skew automatically.\n",
    "\n",
    "Spark handles:\n",
    "- Distributed join execution\n",
    "- Task scheduling and partitioning\n",
    "- Optimizations via AQE when enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "878efab0-2190-43ca-8cc3-c7c476fdc19a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f16d4b-91ba-45e8-a9f2-b89231269526",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read skewed datasets\n",
    "df_a = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    ")\n",
    "\n",
    "df_b = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4dc965-ad5c-4b37-968b-1adf9a77d95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a.printSchema()\n",
    "df_b.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057da55c-6e9b-499a-b035-3162868589c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5d3015-a29d-43e3-a928-3ee0c164df1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_a.limit(5))\n",
    "display(df_b.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0590f251-7825-44f4-adc1-6b056bbd62ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Detecting Skewed Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcfada2-6ca3-41fa-b410-9d766b7b4555",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skew_stats = (\n",
    "    df_a\n",
    "        .groupBy(\"customer_id\")\n",
    "        .count()\n",
    "        .orderBy(F.desc(\"count\"))\n",
    ")\n",
    "\n",
    "skew_stats.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391efe1b-eddf-448f-a72c-1875f7415b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîé What You‚Äôll Observe\n",
    "\n",
    "- A few `customer_id`s appear at the top with **very high counts**\n",
    "- In the Spark UI:\n",
    "  - One or two join tasks take much longer\n",
    "  - These tasks process far more data than others\n",
    "\n",
    "This confirms **data skew**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca9b1a2c-732e-4de3-baed-152dc83d7ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öñÔ∏è Handling Skew with Salting\n",
    "\n",
    "To distribute the load of hot keys across multiple tasks,\n",
    "we use **key salting**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755a8fb9-6276-4aa9-a943-ce27d4f659ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "HOT_CUSTOMERS = [\"cust_hot_1\", \"cust_hot_2\"]\n",
    "N_SALTS = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339e8763-15ac-4df2-a242-c47ca06b9947",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_a_salted = df_a.withColumn(\n",
    "    \"salt\",\n",
    "    F.when(\n",
    "        F.col(\"customer_id\").isin(HOT_CUSTOMERS),\n",
    "        (F.rand() * N_SALTS).cast(\"int\")\n",
    "    ).otherwise(F.lit(0))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c498c1-1e84-4da6-8583-2b840a37d546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_b_salted = df_b.withColumn(\n",
    "    \"salt\",\n",
    "    F.when(\n",
    "        F.col(\"customer_id\").isin(HOT_CUSTOMERS),\n",
    "        (F.rand() * N_SALTS).cast(\"int\")\n",
    "    ).otherwise(F.lit(0))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0e052e8-50cc-42bb-8091-8f5932ff1cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîó Join Using Salted Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdecbcd7-cdea-4eb2-acfd-70547c690a8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = (\n",
    "    df_a_salted.alias(\"a\")\n",
    "        .join(\n",
    "            df_b_salted.alias(\"b\"),\n",
    "            on=[\n",
    "                F.col(\"a.customer_id\") == F.col(\"b.customer_id\"),\n",
    "                F.col(\"a.salt\") == F.col(\"b.salt\")\n",
    "            ],\n",
    "            how=\"inner\"\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cf2aa4-e01e-4815-a1aa-c83263363266",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è About Adaptive Query Execution (AQE)\n",
    "\n",
    "In some Spark environments, **Adaptive Query Execution (AQE)** can automatically\n",
    "detect and mitigate skewed joins at runtime.\n",
    "\n",
    "‚ö†Ô∏è On **Databricks Serverless compute**, Spark execution configurations\n",
    "(such as `spark.sql.adaptive.*`) are **managed by the platform** and\n",
    "cannot be manually enabled or disabled by users.\n",
    "\n",
    "For this reason, we rely on **explicit techniques like key salting**\n",
    "to handle skew in a predictable and portable way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaa0027-a9cc-42a8-ad47-7e8abb297631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Why This Works\n",
    "\n",
    "- Skewed keys are **split across multiple partitions**\n",
    "- No single task is overloaded\n",
    "- Cluster resources are used more evenly\n",
    "- Job runtime improves significantly\n",
    "\n",
    "Salting is a **manual but reliable** solution,  \n",
    "while AQE provides **automatic skew mitigation** in many cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb319122-9d34-454b-9ed2-8879a45ffcff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Data skew is a common cause of slow Spark joins.\n",
    "- Skew can be detected using aggregation and Spark UI.\n",
    "- Key salting spreads hot keys across partitions.\n",
    "- AQE can automatically mitigate skew in supported environments.\n",
    "\n",
    "This notebook demonstrates a **production-grade strategy**\n",
    "for handling skewed joins in large-scale Spark workloads.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Skewed_Key_in_Join_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
