{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a08ce1-efa4-4f9c-8335-92dd4f73de17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0f1d3b-d710-4b46-84e8-6976953c9561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚è±Ô∏è Sliding Window Metrics with Structured Streaming\n",
    "\n",
    "This notebook demonstrates how to compute a **rolling ‚Äúlast 30 minutes error count‚Äù**\n",
    "from application logs using **Spark Structured Streaming**.\n",
    "\n",
    "We simulate real-time ingestion by converting an existing large CSV file\n",
    "into multiple small files and reading them as a **stream**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2747bc09-c748-408e-bde9-446d309c0613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `app_logs_large.csv`  \n",
    "\n",
    "\n",
    "### Columns:\n",
    "- `event_time`\n",
    "- `level`\n",
    "- `service`\n",
    "- `message`\n",
    "\n",
    "> ‚ö†Ô∏è In real production systems, logs arrive continuously via **Kafka**.  \n",
    "> For learning purposes, we simulate streaming using files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0361dff6-75fa-4d2f-9f4f-f432455e21e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "The business wants a **real-time operational metric**:\n",
    "\n",
    "> **‚ÄúHow many ERROR logs occurred in the last 30 minutes?‚Äù**\n",
    "\n",
    "Requirements:\n",
    "- Metric should update continuously\n",
    "- Must be based on **event time**\n",
    "- Must handle **late-arriving events**\n",
    "- Should scale as log volume grows\n",
    "\n",
    "You are asked to implement this using **Spark Structured Streaming**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "1. Read historical log data as batch\n",
    "2. Convert it into a streaming-friendly format\n",
    "3. Read logs as a streaming DataFrame\n",
    "4. Filter ERROR logs\n",
    "5. Apply a **30-minute sliding window** (slide every 1 minute)\n",
    "6. Add a watermark for late data\n",
    "7. Output rolling error counts\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Logs are event-time based\n",
    "- Data may arrive late\n",
    "- Databricks **Serverless compute** is used\n",
    "- Unity Catalog storage is available\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Streaming aggregation computing rolling error counts\n",
    "- Output grouped by:\n",
    "  - window\n",
    "  - service\n",
    "\n",
    "### Expected Columns\n",
    "\n",
    "| window.start | window.end | service | error_count |\n",
    "|--------------|------------|---------|-------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "- Streaming reads **directories**, not single files\n",
    "- New files represent new streaming data\n",
    "- Watermarks control late data handling\n",
    "- Sliding windows enable rolling metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1faa74-c9e2-4c7d-95ca-8e5cad2b4fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read existing logs as a batch DataFrame\n",
    "2. Split batch data into multiple small files\n",
    "3. Read those files using `readStream`\n",
    "4. Filter ERROR events\n",
    "5. Apply sliding window aggregation\n",
    "6. Write streaming results using a serverless-safe trigger\n",
    "\n",
    "Spark handles:\n",
    "- incremental processing\n",
    "- stateful window aggregation\n",
    "- fault tolerance using checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b62862-1160-4cf7-83f2-94a725837ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d916ceae-1e28-41d4-9f5d-b900b728a0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Read Existing Logs (Batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dbf199-ea0f-4e1d-aeda-68ed44370026",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_logs_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    "         .withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    ")\n",
    "\n",
    "display(batch_logs_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992a210b-a19d-4a40-b490-972c7cf4bef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logs_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6f68b1-6773-4cee-b24e-4e2dc5f5fa65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Convert Batch Data into Streaming Input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd184d29-4e0b-4fc2-aa2d-4577cdbec208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    batch_logs_df\n",
    "        .repartition(5)   # simulate data arriving in chunks\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .csv(\"your_directory\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f18d81e-b38a-440f-8767-bc0e9360af1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Read Logs as a Stream\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f56e89-491e-4770-b547-b85d0d866560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logs_df = (\n",
    "    spark.readStream\n",
    "         .schema(\"event_time STRING, level STRING, service STRING, message STRING\")\n",
    "         .option(\"header\", \"true\")\n",
    "         .csv(\"your_directory\")\n",
    "         .withColumn(\"event_time\", F.to_timestamp(\"event_time\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf5e672e-a6df-4b80-8742-082e4c6afdc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üö® Filter ERROR Logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2431f65f-d138-4cd4-ad76-8ce91efb71ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "errors_df = logs_df.filter(F.col(\"level\") == \"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed0c7c81-c586-4078-ae8d-7319a88f367e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚è±Ô∏è Sliding Window Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3f3f50-0c9f-4bf4-8f11-cdc9c5bd7093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowed_errors = (\n",
    "    errors_df\n",
    "        .withWatermark(\"event_time\", \"5 minutes\")\n",
    "        .groupBy(\n",
    "            F.window(\"event_time\", \"30 minutes\", \"1 minute\"),\n",
    "            F.col(\"service\")\n",
    "        )\n",
    "        .agg(F.count(\"*\").alias(\"error_count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b5368d2-2f7e-4384-8286-582bd6485f90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üì§ Write Streaming Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df23ba46-b26e-4192-adaa-faca79e31ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    windowed_errors\n",
    "        .writeStream\n",
    "        .outputMode(\"update\")\n",
    "        .format(\"console\")   # demo output\n",
    "        .option(\"truncate\", \"false\")\n",
    "        .option(\n",
    "            \"checkpointLocation\",\n",
    "            \"your_directory\"\n",
    "        )\n",
    "        .trigger(availableNow=True)   # serverless-safe\n",
    "        .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2093bc6-13ba-4c83-8f48-fb8a44aeb705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† How This Works Internally\n",
    "\n",
    "- Spark tracks state per (window, service)\n",
    "- Each event contributes to multiple overlapping windows\n",
    "- Watermark controls how long Spark waits for late data\n",
    "- Checkpoints ensure fault tolerance and exactly-once semantics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01495f85-272f-42be-9f31-80977ec929d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Sliding windows enable rolling real-time metrics\n",
    "- File-based streaming simulates Kafka ingestion\n",
    "- Watermarks handle late data\n",
    "- Unity Catalog volumes ensure serverless compatibility\n",
    "\n",
    "This pattern is **production-grade** and widely used\n",
    "in monitoring and observability systems.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_Sliding_Window_for_Metrics_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
