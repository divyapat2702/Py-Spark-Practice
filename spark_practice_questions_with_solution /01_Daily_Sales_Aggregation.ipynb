{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87722d4-44d2-4280-aafa-acacfe42be44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f7fe4c8-99e9-4884-b134-59edae74e4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üìÖ Daily Sales Aggregation (Batch)\n",
    "\n",
    "This notebook shows how to design a **daily Spark batch job**\n",
    "that processes **one day‚Äôs sales CSV file**, aggregates total sales\n",
    "by region, and writes the results to a curated table used by BI dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e0a5e8-fe03-4dfd-9d26-50dec3350c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "Daily sales files arrive as **individual CSVs**:\n",
    "\n",
    "- `sales_orders_2025-01-01.csv`\n",
    "- `sales_orders_2025-01-02.csv`\n",
    "- `sales_orders_2025-01-03.csv`\n",
    "- `sales_orders_2025-01-04.csv`\n",
    "- `sales_orders_2025-01-05.csv`\n",
    "\n",
    "**Location (example ‚Äì Databricks Volume)**\n",
    "\n",
    "\n",
    "### Schema\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `region`\n",
    "- `customer_id`\n",
    "- `amount`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6076ac5-2433-4001-b85e-c714a996f8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "Your company receives **one sales CSV per day** in cloud storage\n",
    "(ADLS / Blob Storage or S3).\n",
    "\n",
    "Business requirements:\n",
    "- Process **only the new day‚Äôs file**\n",
    "- Compute **total sales amount per region**\n",
    "- Store results in a **curated table** for BI dashboards\n",
    "- Ensure the job is **safe to re-run** for the same day (idempotent)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Build a Spark batch job that:\n",
    "\n",
    "1. Accepts a `process_date` parameter\n",
    "2. Reads only the corresponding daily CSV file\n",
    "3. Aggregates total sales per region\n",
    "4. Writes results to a curated table\n",
    "5. Overwrites that day‚Äôs data if the job is re-run\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Storage (ADLS / S3 / Volume) is already accessible in Spark\n",
    "- One CSV file exists per day\n",
    "- Job is scheduled daily via Airflow / ADF / cron\n",
    "- BI dashboards read from the curated table\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Daily sales totals per region\n",
    "- Output stored in Delta / Parquet\n",
    "- Data partitioned by `order_date`\n",
    "\n",
    "### Expected Output Schema\n",
    "\n",
    "| order_date | region | total_sales_amount |\n",
    "|------------|--------|--------------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Always **parameterize dates** in batch jobs\n",
    "- Never scan all files when only one day is required\n",
    "- File naming conventions (`YYYY-MM-DD`) are enough for daily jobs\n",
    "- Design jobs to be **idempotent** so re-runs don‚Äôt duplicate data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432951e6-980e-47a8-bdc1-0a41386d5b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Receive `process_date` from scheduler\n",
    "2. Build input file path using the date\n",
    "3. Read only that file\n",
    "4. Aggregate sales by region\n",
    "5. Write output partitioned by `order_date`\n",
    "6. Overwrite the day‚Äôs partition for idempotency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d2193f-21b8-4b1a-ba4b-02d5d8b44cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64a49ded-8127-46d7-88b3-eab78cb4fbc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Job Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e4fc02-0f91-4dc3-92e6-4e7885a1bb11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In production, this comes from Airflow / ADF / scheduler\n",
    "process_date = \"2025-01-02\"\n",
    "\n",
    "# Build input path from the date\n",
    "input_path = f\"your_data\"\n",
    "\n",
    "# Curated output location\n",
    "output_path = \"your_directory\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0bf1f4f-f126-4f58-9300-6e43a4cad559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Read Only That Day‚Äôs File\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2204f5a-91ad-4ba0-bcad-60dc4e3f64f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(input_path)\n",
    ")\n",
    "\n",
    "display(sales_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed57b76c-36f6-44a6-b82b-6550d1d5957a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Aggregation Logic\n",
    "\n",
    "Business Question:  \n",
    "**What is the total sales amount per region for the given day?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dda4877-d2ed-451e-bc3c-9fdde6261a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_df = (\n",
    "    sales_df\n",
    "        .groupBy(\"order_date\", \"region\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales_amount\"))\n",
    ")\n",
    "\n",
    "display(agg_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22ecfe08-2733-4080-9569-6f9886855056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üíæ Write to Curated Layer (Idempotent)\n",
    "\n",
    "We overwrite **only the current day‚Äôs data**\n",
    "so the job can be safely re-run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddbe9b90-6722-4c29-8ac7-bd914957b2cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    agg_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .partitionBy(\"order_date\")\n",
    "        .format(\"delta\")          # parquet also acceptable\n",
    "        .save(output_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a94d24-6e1b-4f17-b67f-2ebb2a68a379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÅ What Happens on Re-run?\n",
    "\n",
    "- Same `process_date`\n",
    "- Same input file\n",
    "- Same output partition\n",
    "- Old data is replaced\n",
    "\n",
    "‚úÖ No duplicates  \n",
    "‚úÖ Safe reprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e2c4d96-4cc2-4687-9913-25de0ae058f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Daily batch jobs should be **date-driven**\n",
    "- Read less data, not more\n",
    "- Partition outputs for BI performance\n",
    "- Always design for safe re-runs\n",
    "\n",
    "This is a **production-grade Spark batch pattern**\n",
    "used across real data platforms.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Daily_Sales_Aggregation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
