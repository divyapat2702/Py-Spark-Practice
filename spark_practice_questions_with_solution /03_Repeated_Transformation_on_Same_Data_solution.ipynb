{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a8bf0d-fcce-4310-9718-cfc9f14e6242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8f7c62-70fd-435a-8776-49317602a3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üîÅ Reusing Computation Efficiently in Spark\n",
    "\n",
    "This notebook demonstrates how Apache Spark can **avoid recomputing the same expensive transformations**\n",
    "when the **same cleaned DataFrame is used multiple times** within a single job.\n",
    "\n",
    "We focus on **performance optimization** using Spark‚Äôs **cache / persist** mechanism,\n",
    "which is critical when working with **large datasets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36aa341-aa7a-49ce-b817-90b82752ad85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Primary Dataset:** `sales_orders_raw_with_issues.csv`  \n",
    "**Optional Large Dataset:** `sales_orders_large.csv`\n",
    "\n",
    "> ‚ö†Ô∏è In real-world scenarios, sales datasets can be **very large (GBs or more)**.  \n",
    "To keep this exercise practical, we assume the dataset already exists in  \n",
    "**your catalog / database storage**.\n",
    "\n",
    "### Example Columns:\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `customer_id`\n",
    "- `region`\n",
    "- `category`\n",
    "- `amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19cb669-0d7d-4214-aa68-20adec5ae542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are working with a **sales orders dataset** that requires multiple cleaning steps\n",
    "before it can be used for reporting.\n",
    "\n",
    "After cleaning, the same cleaned DataFrame is used to generate **5 different reports**\n",
    "inside the **same Spark job**.\n",
    "\n",
    "Without optimization, Spark will **re-run the same cleaning transformations**\n",
    "every time a report is computed, leading to:\n",
    "- unnecessary recomputation\n",
    "- increased job runtime\n",
    "- wasted cluster resources\n",
    "\n",
    "Your goal is to **optimize the job** so that the cleaning logic runs **only once**.\n",
    "\n",
    "The input data already exists in **your catalog / database storage** and needs to be\n",
    "cleaned, reused, and analyzed efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps using Spark:\n",
    "\n",
    "1. **Read** the raw sales orders dataset.\n",
    "2. Apply all **cleaning and standard transformations** to create a cleaned DataFrame.\n",
    "3. **Cache or persist** the cleaned DataFrame so Spark materializes it once.\n",
    "4. Reuse the cleaned DataFrame to generate **multiple reports**.\n",
    "5. **Unpersist** the DataFrame after all reports are completed to free resources.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- The raw dataset contains data quality issues (nulls, duplicates, invalid values).\n",
    "- The cleaned DataFrame is reused multiple times within the same Spark job.\n",
    "- Spark uses **lazy evaluation**, so transformations are not executed until an action occurs.\n",
    "- The dataset is large enough that recomputation would be expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- **Cleaned DataFrame:** reused across multiple reports  \n",
    "- **Reports:** Aggregations derived from the same cached DataFrame  \n",
    "\n",
    "### **Example Reports**\n",
    "- Total sales by region\n",
    "- Total sales by category\n",
    "- Daily sales trends\n",
    "- (Additional reports can be added without re-running cleaning logic)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Spark does **not** automatically remember intermediate results.\n",
    "- Without caching, Spark **recomputes transformations for every action**.\n",
    "- Caching is useful when:\n",
    "  - a DataFrame is expensive to compute\n",
    "  - the same DataFrame is reused multiple times\n",
    "- Always unpersist cached data when it is no longer needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51aa26f7-c116-46ed-b481-b96e8f0b3bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. **Read the raw sales orders dataset** from your catalog / database storage using Spark.\n",
    "2. Apply all **data cleaning and standard transformations** once to create a cleaned DataFrame (`clean_df`).\n",
    "3. **Cache or persist** the cleaned DataFrame so Spark stores the computed result after the first action.\n",
    "4. Trigger an **action** (such as `count()` or the first report) to materialize the cached DataFrame.\n",
    "5. Reuse the cached `clean_df` across **multiple reports** (aggregations, groupings, joins).\n",
    "6. After all reports are generated, **unpersist** the DataFrame to free up cluster memory.\n",
    "\n",
    "Spark handles:\n",
    "- Avoiding repeated recomputation of expensive transformations\n",
    "- Efficient reuse of intermediate results\n",
    "- Memory management for cached data\n",
    "- Parallel execution across executors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74ff4dc-6a8c-4157-9d7a-234a93ac8f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_path = \"your_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5911fe26-f625-4107-8f25-5351dcc24f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(input_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "795ec586-9712-4ad0-b93d-e01f7dba9dc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ee566fc-8a48-47e3-8565-8ec91006aa71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Input Data (Raw)\n",
    "\n",
    "This dataset may contain:\n",
    "- null customer IDs\n",
    "- invalid or negative amounts\n",
    "- missing categories\n",
    "- duplicate records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365d553c-a03b-4c36-83b9-97fc160711bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(raw_df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3baf65e4-8164-4b7a-b610-4f374579b874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßπ Cleaning & Standardization\n",
    "\n",
    "We apply all expensive transformations **once**\n",
    "to create a cleaned DataFrame that will be reused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f236a44f-74f0-4b07-8618-2ebfb42f96ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_df = (\n",
    "    raw_df\n",
    "        .filter(F.col(\"customer_id\").isNotNull())\n",
    "        .filter(F.col(\"amount\") > 0)\n",
    "        .filter(F.col(\"category\").isNotNull())\n",
    "        .dropDuplicates()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f26b071f-6d9f-464e-9a7b-8f38590ac0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üöÄ Performance Optimization: Cache the Cleaned Data\n",
    "\n",
    "To avoid recomputing the same transformations for every report,\n",
    "we **cache** the cleaned DataFrame.\n",
    "\n",
    "‚ö†Ô∏è The data is cached **only after the first action is triggered**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e506f355-87f9-4728-9b92-b369061e02e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "temp_clean_path = \"your_directory\"\n",
    "\n",
    "(\n",
    "    clean_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"delta\")\n",
    "        .save(temp_clean_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ba84552-5f5f-4633-9a87-262c1df2285d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_df = spark.read.format(\"delta\").load(temp_clean_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6302aa-bfba-4760-b4ab-aaaf307ece49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìä Reports Using the Cached DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d897aa65-16c0-4d7e-b513-21f0b244df29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Report 1: Total sales by region\n",
    "report_region = (\n",
    "    clean_df\n",
    "        .groupBy(\"region\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales\"))\n",
    ")\n",
    "\n",
    "display(report_region)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e524109-9e2a-4d1f-a805-6ede61c44503",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Report 2: Total sales by category\n",
    "report_category = (\n",
    "    clean_df\n",
    "        .groupBy(\"category\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales\"))\n",
    ")\n",
    "\n",
    "display(report_category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6093da17-a988-417e-89eb-96d93ec5974d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Report 3: Daily sales\n",
    "report_daily = (\n",
    "    clean_df\n",
    "        .groupBy(\"order_date\")\n",
    "        .agg(F.sum(\"amount\").alias(\"daily_sales\"))\n",
    ")\n",
    "\n",
    "display(report_daily)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e10934c2-f38d-4d1a-886e-ac5d2ad01656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Why Caching Helps Here\n",
    "\n",
    "- Cleaning logic runs **only once**\n",
    "- All reports reuse the **materialized DataFrame**\n",
    "- Significant performance improvement for large datasets\n",
    "- Reduced load on the Spark cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b016230-64de-441e-a7bc-c2143d191999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üßπ Cleanup (Serverless)\n",
    "\n",
    "On Databricks Serverless compute, intermediate DataFrames\n",
    "cannot be cached or unpersisted.\n",
    "\n",
    "If cleanup is required, the temporary Delta data can be:\n",
    "- overwritten in the next run, or\n",
    "- deleted explicitly if no longer needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ba5380-1f97-4535-8522-2b6585c7c1d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(temp_clean_path, recurse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8519b75-a3b2-470d-85fc-b31e94f13e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Spark uses **lazy evaluation**, which can cause repeated recomputation.\n",
    "- When the same DataFrame is reused multiple times, **cache or persist it**.\n",
    "- Caching improves performance and reduces resource usage.\n",
    "- Always unpersist cached data after use.\n",
    "\n",
    "This pattern is **widely used in real-world Spark pipelines**\n",
    "and is critical for building efficient, scalable data applications.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Repeated_Transformation_on_Same_Data_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
