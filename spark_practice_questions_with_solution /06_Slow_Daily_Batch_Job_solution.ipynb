{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a93e3e6-2b12-48ae-a22a-10ff03fb21c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d2256c-46e1-40a7-b4a1-33822747ac3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üê¢ Optimizing a Slow Daily Spark Batch Job\n",
    "\n",
    "This notebook demonstrates how to **diagnose and optimize a slow-running daily Spark batch job**\n",
    "using a combination of **Spark UI analysis** and **code-level optimizations**.\n",
    "\n",
    "The job aggregates **daily sales by region** and has gradually slowed down\n",
    "as data volume increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6720c2b-d85b-489b-81cb-dd877bd1fbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `sales_orders_large.csv`  \n",
    "**Description:** 30 days of historical sales data\n",
    "\n",
    "**Example Columns:**\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `region`\n",
    "- `customer_id`\n",
    "- `category`\n",
    "- `quantity`\n",
    "- `amount`\n",
    "\n",
    "The dataset is assumed to be available in **your catalog / database storage**.\n",
    "\n",
    "> ‚ö†Ô∏è In real production systems, this dataset typically grows every day,\n",
    "which can cause batch jobs to slow down if not designed correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3447cc0d-1646-436e-ab70-8b5f21e908d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You own a **daily Spark batch job** that aggregates total sales by region.\n",
    "\n",
    "Initially:\n",
    "- Runtime ‚âà **15 minutes**\n",
    "\n",
    "Over time:\n",
    "- Runtime slowly increased to **2 hours**\n",
    "\n",
    "No single change caused the slowdown ‚Äî instead, it happened gradually as:\n",
    "- Data volume increased\n",
    "- More historical data accumulated\n",
    "- Inefficient read patterns became more expensive\n",
    "- Shuffle-heavy operations started processing much more data\n",
    "\n",
    "This is a **very common real-world problem** in batch data pipelines.\n",
    "\n",
    "Your goal is to:\n",
    "- **Diagnose** the performance bottlenecks using Spark UI\n",
    "- **Optimize** the job so it continues to scale as data grows\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "1. Use **Spark UI** to identify where the job is slow.\n",
    "2. Determine whether the job is:\n",
    "   - reading too much data\n",
    "   - performing large or unnecessary shuffles\n",
    "   - affected by data skew\n",
    "3. Apply **code-level optimizations** to:\n",
    "   - reduce the amount of data scanned\n",
    "   - reduce shuffle cost during aggregation\n",
    "   - use efficient storage formats\n",
    "4. Validate that the optimized job processes **only the required day‚Äôs data**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- The job runs **once per day** for a single `process_date`.\n",
    "- Historical sales data is stored in **cloud object storage**.\n",
    "- Data volume **grows continuously** as new days are added.\n",
    "- Spark Serverless or classic Spark clusters may be used.\n",
    "- No changes were made to business logic ‚Äî only data volume increased.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- An **optimized daily aggregation job** that scales with growing data.\n",
    "- Aggregated output containing **total sales by region for a single day**.\n",
    "- A clear demonstration of **performance improvement** compared to the naive approach.\n",
    "\n",
    "### **Expected Output Columns**\n",
    "\n",
    "| order_date | region | total_sales_amount |\n",
    "|-----------|--------|--------------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes \n",
    "\n",
    "- Performance problems usually appear **gradually** as data grows, not suddenly.\n",
    "- Always start optimization by **observing Spark UI**, not changing code blindly.\n",
    "- The biggest performance wins usually come from **reading less data**, not adding more compute.\n",
    "- Using **Parquet or Delta** enables:\n",
    "  - column pruning\n",
    "  - predicate pushdown\n",
    "- Partitioning data by frequently filtered columns (such as `order_date`) enables **partition pruning**.\n",
    "- Code that works well for small datasets may **fail to scale** without these optimizations.\n",
    "\n",
    "> üí° Tip:  \n",
    "> If a batch job becomes slower over time, assume a **data growth and data layout problem first**, not a Spark bug.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb71877a-8d91-444b-96b7-115c1eaee804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Use **Spark UI** to identify slow stages and expensive operations.\n",
    "2. Confirm whether the job is scanning **more data than necessary**.\n",
    "3. Convert raw CSV data into an **efficient columnar format** (Parquet or Delta).\n",
    "4. Ensure the dataset is **partitioned by order_date**.\n",
    "5. Modify the job to:\n",
    "   - read only the required date partition\n",
    "   - select only necessary columns\n",
    "6. Reduce shuffle overhead during aggregation.\n",
    "7. Write optimized output for downstream consumption.\n",
    "\n",
    "Spark handles:\n",
    "- Distributed execution\n",
    "- Partition pruning\n",
    "- Column pruning\n",
    "- Shuffle-based aggregations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a8dfd6-9ca2-425a-a3fa-61c47d473095",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77667500-4d26-4877-8b04-fd3cd0501236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß± One-Time Setup: Raw ‚Üí Parquet (Recommended)\n",
    "\n",
    "This conversion is done **once**, not daily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6817042-b35f-410e-9eb0-369aa70db505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    ")\n",
    "\n",
    "(\n",
    "    raw_df.write\n",
    "          .mode(\"overwrite\")\n",
    "          .partitionBy(\"order_date\")\n",
    "          .parquet(\"your_directory\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60724ce9-43ab-4ae1-ab49-15173bd52ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f83d68f-7d94-4787-b893-b0027ada626f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(raw_df.limit(5))     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08cfd9f-1dc4-4b15-8784-11df7d620836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùå Naive (Slow) Daily Job\n",
    "\n",
    "This version represents how many batch jobs are initially written.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aa8bd72-d2f2-4345-8dd6-73f8a03449e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_date = \"2025-01-20\"\n",
    "\n",
    "# ‚ùå BAD: reads all data, all columns, CSV format\n",
    "sales_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    ")\n",
    "\n",
    "agg_df = (\n",
    "    sales_df\n",
    "        .groupBy(\"region\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales_amount\"))\n",
    ")\n",
    "\n",
    "agg_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ca3e21b-118f-4f73-9585-029c6820f12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚ùå Problems with the Naive Version\n",
    "\n",
    "- Reads **all historical data**, not just one day\n",
    "- Uses **CSV** (no predicate pushdown, no column pruning)\n",
    "- Triggers a **large shuffle** during aggregation\n",
    "- Job runtime increases as data grows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9889a2-7d31-49cb-a193-53f77a680c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Diagnosing the Problem Using Spark UI\n",
    "\n",
    "When analyzing this job in Spark UI, look for:\n",
    "\n",
    "### Stages Tab\n",
    "- One or more stages taking significantly longer\n",
    "- Aggregation (`groupBy`) stages causing wide shuffles\n",
    "\n",
    "### Tasks Tab\n",
    "- Tasks with very large input sizes\n",
    "- Long-running tasks compared to others (possible skew)\n",
    "\n",
    "### SQL / Query Tab\n",
    "- Full table scan instead of partition pruning\n",
    "- Large shuffle read/write sizes\n",
    "\n",
    "This confirms the job is **doing more work than necessary**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e46f6d0d-4256-4390-bdd6-e4b69c148a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Optimized Daily Job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b36e5e-a19e-4569-994d-ae464a80239f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_date = \"2025-01-20\"\n",
    "\n",
    "# ‚úÖ Read only required partition and columns from Parquet\n",
    "sales_df = (\n",
    "    spark.read\n",
    "         .parquet(\"your_directory\")\n",
    "         .where(F.col(\"order_date\") == process_date)\n",
    "         .select(\"order_date\", \"region\", \"amount\")\n",
    ")\n",
    "\n",
    "agg_df = (\n",
    "    sales_df\n",
    "        .groupBy(\"order_date\", \"region\")\n",
    "        .agg(F.sum(\"amount\").alias(\"total_sales_amount\"))\n",
    ")\n",
    "\n",
    "display(agg_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4770d253-43e9-4af3-b28c-c28b31bfe8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üíæ Writing Optimized Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e079f0e7-ad02-41b9-8fee-8d33970f0d5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    agg_df.write\n",
    "          .mode(\"overwrite\")\n",
    "          .partitionBy(\"order_date\")\n",
    "          .format(\"delta\")\n",
    "          .save(\"your_directory\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6e2098a-2b98-458c-97ca-3a3f4461c01f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Why This Is Faster\n",
    "\n",
    "- **Partition pruning**: only one day‚Äôs data is read\n",
    "- **Column pruning**: only required columns are scanned\n",
    "- **Parquet/Delta**: efficient columnar reads\n",
    "- **Reduced shuffle size**: less data moved across the network\n",
    "- Job runtime remains stable even as data grows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1866bf27-4fde-4d40-9860-0e6cbbac6d8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Additional Optimizations (Conceptual)\n",
    "\n",
    "Depending on the workload, you may also:\n",
    "- Tune shuffle partitions (on classic clusters)\n",
    "- Broadcast small dimension tables (if joins exist)\n",
    "- Handle skewed keys if one region dominates\n",
    "- Remove repeated expensive transformations\n",
    "\n",
    "‚ö†Ô∏è On **Databricks Serverless**, execution configs are platform-managed,\n",
    "so focus on **data layout and query design**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b377f1d-1272-4ecb-9b2b-0f3a6512fb7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Slow batch jobs usually degrade due to **data growth**, not bugs.\n",
    "- Spark UI is the first tool to identify performance bottlenecks.\n",
    "- Reading less data is the **biggest optimization**.\n",
    "- Partitioning and columnar formats are critical for scalable batch jobs.\n",
    "- This pattern is widely used in **production data platforms**.\n",
    "\n",
    "This notebook demonstrates a **real-world, industry-standard approach**\n",
    "to diagnosing and optimizing slow Spark batch workloads.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Slow_Daily_Batch_Job_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
