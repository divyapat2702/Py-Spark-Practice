{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fca92834-39c4-4832-a6a3-963345c36a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b825085-0e4a-437a-a2e1-173a15a874d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚öñÔ∏è Dynamic Resource Allocation in a Shared Cluster\n",
    "\n",
    "This notebook demonstrates how to configure **Spark Dynamic Resource Allocation**\n",
    "so a job can **scale up when busy** and **release resources when idle**\n",
    "in a **shared YARN / Kubernetes cluster**.\n",
    "\n",
    "We use a **shuffle-heavy aggregation** on `big_events_50k.csv`\n",
    "to illustrate how Spark dynamically adjusts executors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28a4d61e-7927-4369-87ea-913d17d42a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `big_events_50k.csv`  \n",
    "\n",
    "### Example Columns:\n",
    "- `event_id`\n",
    "- `event_time`\n",
    "- `country`\n",
    "- `device`\n",
    "- `amount`\n",
    "\n",
    "The dataset is large enough to:\n",
    "- create **shuffle-heavy stages**\n",
    "- demonstrate executor scaling behavior\n",
    "\n",
    "> ‚ö†Ô∏è In real production systems, this dataset could be **hundreds of GBs**.\n",
    "> The same dynamic allocation pattern applies regardless of size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f7517e-b501-411e-a88b-ba950bb0a112",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "Your Spark job runs in a **shared cluster** (YARN or Kubernetes)\n",
    "used by multiple teams.\n",
    "\n",
    "Observed issues:\n",
    "- Sometimes your job **consumes too many executors**\n",
    "- Sometimes it **runs slowly due to lack of resources**\n",
    "- Static executor allocation causes:\n",
    "  - wasted resources when the job is idle\n",
    "  - unfair usage during peak times\n",
    "\n",
    "You want Spark to:\n",
    "- automatically scale up during heavy processing\n",
    "- release executors when work finishes\n",
    "- avoid impacting other teams‚Äô jobs\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Using `big_events_50k.csv`, design a Spark job that:\n",
    "\n",
    "1. Enables **dynamic executor allocation**\n",
    "2. Sets sensible **min / initial / max executors**\n",
    "3. Safely handles shuffle data when executors are removed\n",
    "4. Releases idle executors automatically\n",
    "5. Works well in a **multi-tenant cluster**\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Spark runs on **YARN or Kubernetes**\n",
    "- Cluster is shared by multiple teams\n",
    "- The job contains **shuffle-heavy operations**\n",
    "- Spark configs are set via:\n",
    "  - `spark-submit`\n",
    "  - cluster defaults\n",
    "  - or SparkSession builder\n",
    "\n",
    "> ‚ö†Ô∏è Databricks Serverless manages resources automatically\n",
    "> and does **not allow manual dynamic allocation configuration**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- A Spark job using **dynamic resource allocation**\n",
    "- Executors scale up during shuffle-heavy stages\n",
    "- Executors are released when idle\n",
    "- Job avoids starving other teams\n",
    "\n",
    "### Expected Behavior\n",
    "\n",
    "| Workload State | Executor Behavior |\n",
    "|---------------|-------------------|\n",
    "High shuffle load | Executors scale up |\n",
    "Idle periods | Executors released |\n",
    "Shared cluster | Fair resource usage |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Dynamic allocation is most useful in **shared clusters**\n",
    "- Spark decides executor count **at runtime**\n",
    "- Shuffle handling is critical for safe executor removal\n",
    "- This is a **configuration-level optimization**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19c8f072-b59c-42f8-8803-c10416fa5227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Enable Spark dynamic allocation\n",
    "2. Configure min, initial, and max executors\n",
    "3. Enable safe shuffle handling:\n",
    "   - external shuffle service (YARN)\n",
    "   - shuffle tracking (Kubernetes)\n",
    "4. Tune idle and backlog timeouts\n",
    "5. Run a shuffle-heavy aggregation on `big_events_50k.csv`\n",
    "\n",
    "Spark automatically:\n",
    "- adds executors when tasks back up\n",
    "- removes executors after idle timeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd42041-b53d-4a61-8242-e844aa2747df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8b0a653-6042-434e-b0ce-a4f7724fbf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Spark Configuration (Conceptual ‚Äì Shared Cluster)\n",
    "\n",
    "‚ö†Ô∏è These settings apply to **YARN / Kubernetes clusters**.  \n",
    "They are typically set via `spark-submit` or cluster config.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aafa871a-9c8d-4104-87e1-28632a3390bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"dynamic-allocation-big-events\")\n",
    "\n",
    "        # Enable dynamic allocation\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "\n",
    "        # Executor bounds\n",
    "        .config(\"spark.dynamicAllocation.minExecutors\", \"2\")\n",
    "        .config(\"spark.dynamicAllocation.initialExecutors\", \"4\")\n",
    "        .config(\"spark.dynamicAllocation.maxExecutors\", \"20\")\n",
    "\n",
    "        # Backlog & idle handling\n",
    "        .config(\"spark.dynamicAllocation.schedulerBacklogTimeout\", \"5s\")\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\")\n",
    "\n",
    "        # Kubernetes (use this)\n",
    "        .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\")\n",
    "\n",
    "        # YARN alternative (use instead of shuffleTracking)\n",
    "        # .config(\"spark.shuffle.service.enabled\", \"true\")\n",
    "\n",
    "        .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e749fed-960c-4645-b5d8-9de9fee58396",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be78c9b2-9fd3-41ed-90eb-5ad455380de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    ")\n",
    "\n",
    "display(events_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a21c51c-ee34-484c-b73c-92e5d9783f0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Shuffle-Heavy Aggregation\n",
    "\n",
    "This aggregation forces a **wide shuffle**, which allows us to\n",
    "observe dynamic executor scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3d1428-6c10-4495-b599-c45bf6ebf0b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_df = (\n",
    "    events_df\n",
    "        .groupBy(\"country\", \"device\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"event_count\"),\n",
    "            F.sum(\"amount\").alias(\"total_amount\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Action to trigger execution\n",
    "agg_df.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdb398e1-56be-415e-8042-6b512057d1bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç What Happens at Runtime\n",
    "\n",
    "- Spark starts with **initialExecutors**\n",
    "- If shuffle tasks queue up:\n",
    "  - Spark requests more executors (up to maxExecutors)\n",
    "- After tasks complete:\n",
    "  - Idle executors are released after executorIdleTimeout\n",
    "- Shuffle tracking / external shuffle service ensures safety\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d2c23b4-c265-400d-8c56-44127f218034",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Shared Cluster Best Practices\n",
    "\n",
    "- Set a **reasonable maxExecutors**\n",
    "- Use **fair scheduler / queues**\n",
    "- Avoid large static executor counts\n",
    "- Let Spark adapt to workload size\n",
    "\n",
    "Dynamic allocation benefits everyone in the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b823a813-1973-48d7-bd1a-8c4af47c2452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- `big_events_50k.csv` creates shuffle-heavy workloads\n",
    "- Dynamic allocation enables elastic executor scaling\n",
    "- Idle executors are released automatically\n",
    "- Best suited for shared YARN / Kubernetes clusters\n",
    "\n",
    "This is a **production-grade pattern** for multi-tenant Spark environments.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_Dynamic_Resource_Allocation_in_Shared_Cluster",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
