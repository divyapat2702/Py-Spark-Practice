{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c7f4a6f-d400-497b-ad3e-18ac141007dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a582ba36-693e-4ffd-bf48-f697c432fabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üß¨ Serialization & Cross-Language UDFs\n",
    "\n",
    "This notebook demonstrates how **heavy Python UDF usage** can slow down Spark jobs\n",
    "and how to redesign the pipeline using:\n",
    "\n",
    "- Built-in Spark SQL functions\n",
    "- Vectorized (Pandas) UDFs with Arrow\n",
    "- Efficient serialization formats (Avro)\n",
    "\n",
    "The goal is to **improve performance** and **enable clean interoperability**\n",
    "with a downstream microservice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "151e5b8e-661b-4f28-a864-7d15ad997fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `events_for_udf_50k.csv` \n",
    "\n",
    "### Example Columns\n",
    "- `user_id`\n",
    "- `country`\n",
    "- `segment`\n",
    "- `event_type`\n",
    "- `amount`\n",
    "\n",
    "This dataset simulates user activity events used to compute\n",
    "a **risk / engagement score**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb03381-3647-4dfb-b88b-20f76e1fa822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "Your Spark job:\n",
    "- Processes a medium-sized events dataset\n",
    "- Uses **Python UDFs heavily**\n",
    "- Runs noticeably slower than equivalent Scala jobs\n",
    "\n",
    "Additionally:\n",
    "- The processed data must be sent to a **downstream microservice**\n",
    "- The microservice expects **Avro-encoded data**\n",
    "\n",
    "You want to:\n",
    "- Reduce Python‚ÄìJVM serialization overhead\n",
    "- Improve Spark execution performance\n",
    "- Use a standardized serialization format for interoperability\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Redesign the job to:\n",
    "\n",
    "1. Minimize Python UDF usage\n",
    "2. Prefer Spark SQL / built-in expressions\n",
    "3. Use vectorized UDFs only if necessary\n",
    "4. Keep data columnar inside Spark\n",
    "5. Serialize final output in **Avro**\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Dataset fits comfortably in Spark but has enough rows to show UDF overhead\n",
    "- Business logic computes a per-row engagement score\n",
    "- Microservice understands Avro schema\n",
    "- Cluster is shared and performance-sensitive\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Optimized engagement score computation\n",
    "- Reduced serialization overhead\n",
    "- Avro output for downstream systems\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "| Area | Result |\n",
    "|----|----|\n",
    "Python UDF overhead | Reduced |\n",
    "Execution speed | Improved |\n",
    "Serialization | Standardized (Avro) |\n",
    "Interoperability | Simplified |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- **Python UDFs are slow** because each row must move between the JVM and Python.\n",
    "- Spark SQL / DataFrame expressions run **inside the JVM** and benefit from:\n",
    "  - whole-stage code generation\n",
    "  - vectorized execution\n",
    "- **Always try built-in functions first** before writing any UDF.\n",
    "- If custom logic is unavoidable:\n",
    "  - Prefer **Pandas (vectorized) UDFs** over normal Python UDFs.\n",
    "  - Pandas UDFs use **Apache Arrow**, which reduces serialization overhead.\n",
    "- Keep data in **columnar formats (Parquet / Delta)** while processing inside Spark.\n",
    "- **Serialize only once at system boundaries** (for example, when sending data to a microservice).\n",
    "- **Avro is ideal for cross-language systems** because:\n",
    "  - it enforces schema\n",
    "  - it is compact and fast\n",
    "  - it supports schema evolution\n",
    "- A common anti-pattern is:\n",
    "  - heavy Python UDFs + CSV/JSON everywhere\n",
    "- A common production pattern is:\n",
    "  - Spark SQL / Scala logic ‚Üí columnar storage ‚Üí Avro at the boundary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "920e7817-64dd-4f93-ac18-851e4147168f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Identify Python UDF bottlenecks\n",
    "2. Replace row-based UDFs with Spark SQL expressions\n",
    "3. Use Pandas UDFs only if custom Python logic is unavoidable\n",
    "4. Keep data in columnar formats inside Spark\n",
    "5. Serialize once at the boundary using Avro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fba57f24-7df5-4e0e-b156-5cf6c0ef7465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "711ae3c2-897a-4a8c-866a-bf1e9390a2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bbd1879-0e54-4a25-9c38-f29076ca7053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "events = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"/your_data\")\n",
    ")\n",
    "\n",
    "display(events.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5860aa16-9e4a-4931-a0b9-f919cef25e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚ùå Naive Approach: Heavy Python UDF (Slow)\n",
    "\n",
    "This approach:\n",
    "- Executes Python code **row by row**\n",
    "- Requires Python ‚Üî JVM serialization for every record\n",
    "- Does not benefit from Spark code generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b94371c0-9395-4059-81e7-2143f928d2c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def python_score(segment, event_type, amount):\n",
    "    score = 0.0\n",
    "\n",
    "    if segment == \"churn_risk\":\n",
    "        score += 5\n",
    "    elif segment == \"active\":\n",
    "        score += 2\n",
    "\n",
    "    if event_type == \"purchase\":\n",
    "        score += 10\n",
    "    elif event_type == \"add_to_cart\":\n",
    "        score += 4\n",
    "\n",
    "    score += float(amount) / 50.0\n",
    "    return score\n",
    "\n",
    "score_udf = udf(python_score, DoubleType())\n",
    "\n",
    "scored_naive = events.withColumn(\n",
    "    \"engagement_score\",\n",
    "    score_udf(\"segment\", \"event_type\", \"amount\")\n",
    ")\n",
    "\n",
    "scored_naive.limit(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "945289b9-4836-4739-8281-7cf92d070449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Optimized Approach: Spark SQL Expressions\n",
    "\n",
    "This version:\n",
    "- Runs entirely inside the JVM\n",
    "- Uses whole-stage code generation\n",
    "- Avoids Python serialization overhead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b0e4347-5ecc-4e06-a35b-10f987e58c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scored_expr = (\n",
    "    events\n",
    "        .withColumn(\n",
    "            \"engagement_score\",\n",
    "            F.when(F.col(\"segment\") == \"churn_risk\", F.lit(5.0))\n",
    "             .when(F.col(\"segment\") == \"active\", F.lit(2.0))\n",
    "             .otherwise(F.lit(0.0))\n",
    "            +\n",
    "            F.when(F.col(\"event_type\") == \"purchase\", F.lit(10.0))\n",
    "             .when(F.col(\"event_type\") == \"add_to_cart\", F.lit(4.0))\n",
    "             .otherwise(F.lit(0.0))\n",
    "            +\n",
    "            (F.col(\"amount\") / F.lit(50.0))\n",
    "        )\n",
    ")\n",
    "\n",
    "display(scored_expr.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "758072ae-c566-4dd6-a941-87c27615d493",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üü° Alternative: Pandas (Vectorized) UDF\n",
    "\n",
    "Use **only if custom Python logic cannot be expressed in SQL**.\n",
    "\n",
    "Pandas UDFs:\n",
    "- Process data in batches\n",
    "- Use Apache Arrow for efficient serialization\n",
    "- Are faster than normal Python UDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e273de-9e0a-445c-b29a-abec1be1426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "@pandas_udf(DoubleType())\n",
    "def pandas_score(segment: pd.Series,\n",
    "                 event_type: pd.Series,\n",
    "                 amount: pd.Series) -> pd.Series:\n",
    "\n",
    "    base = segment.map(\n",
    "        {\"churn_risk\": 5.0, \"active\": 2.0}\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    event_bonus = event_type.map(\n",
    "        {\"purchase\": 10.0, \"add_to_cart\": 4.0}\n",
    "    ).fillna(0.0)\n",
    "\n",
    "    return base + event_bonus + (amount.astype(float) / 50.0)\n",
    "\n",
    "scored_pandas = events.withColumn(\n",
    "    \"engagement_score\",\n",
    "    pandas_score(\"segment\", \"event_type\", \"amount\")\n",
    ")\n",
    "\n",
    "display(scored_pandas.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37b7c99b-8345-407f-a500-15f42ca206b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Comparison Summary\n",
    "\n",
    "| Approach | Performance | Notes |\n",
    "|------|-----------|------|\n",
    "Python UDF | ‚ùå Slow | Per-row execution |\n",
    "Pandas UDF | ‚ö†Ô∏è Medium | Vectorized, Arrow-based |\n",
    "Spark SQL | ‚úÖ Fastest | JVM + codegen |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2e767a7-b9de-4058-b51d-0a423a7af133",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üì¶ Preparing Data for Microservice (Avro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6d2a73-8c4e-48a3-b3b0-a2fb245cc42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    scored_expr\n",
    "        .select(\n",
    "            \"user_id\",\n",
    "            \"country\",\n",
    "            \"segment\",\n",
    "            \"event_type\",\n",
    "            \"amount\",\n",
    "            \"engagement_score\"\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ded7a1a8-fd5b-4f85-92f8-9cbea7dca45f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üíæ Writing Output as Avro\n",
    "\n",
    "Avro provides:\n",
    "- Compact binary format\n",
    "- Schema enforcement\n",
    "- Cross-language compatibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb65e43-7e01-4d73-806d-af8b6c5ad89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    final_df\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"avro\")\n",
    "        .save(\"your_directory\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4629d547-1bb2-4495-975e-3cdf4617a2c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Why Avro Works Well Here\n",
    "\n",
    "- Spark writes Avro natively\n",
    "- Microservices can read Avro directly\n",
    "- One schema shared across systems\n",
    "- No JSON / CSV parsing overhead\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "611366e7-5400-4d35-a088-191c5fdfd336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Python UDFs are expensive due to serialization\n",
    "- Spark SQL expressions are fastest\n",
    "- Pandas UDFs are a good compromise when needed\n",
    "- Avro enables clean cross-language interoperability\n",
    "\n",
    "This design improves **performance**, **scalability**, and **system integration**.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12_Serialization_&_Cross_Language_UDFs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
