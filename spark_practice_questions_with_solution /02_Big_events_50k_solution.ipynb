{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc5bb57-f303-4189-bab0-47d6f313743a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e51851-39b3-4e96-a2c5-89970bdbaf52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üì¶ Big Data, Small Machines\n",
    "\n",
    "This notebook demonstrates how Apache Spark on Databricks can process\n",
    "datasets that are **too large to fit into the memory of a single machine**.\n",
    "\n",
    "We simulate a **500 GB CSV file** scenario using a smaller dataset,\n",
    "while following the **exact same processing pattern** used for large-scale data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b53e3e-dc2e-405d-9b08-cb7dd73741e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `big_events_50k.csv`  \n",
    "**Storage Location:** Databricks File System (DBFS) (Refer Create a Databricks Catalog and Upload a CSV.pdf)\n",
    "\n",
    "> ‚ö†Ô∏è In real-world scenarios, this dataset would be a **500 GB CSV**\n",
    "stored in **ADLS / S3**.  \n",
    "Since we cannot upload such a large file here, we use **50k rows to simulate the same pattern**.\n",
    "\n",
    "### Example Columns:\n",
    "- `event_id`\n",
    "- `event_time`\n",
    "- `country`\n",
    "- `device`\n",
    "- `amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be20d76f-05cc-4085-aa6d-4edb8d72722f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are working with a **very large events dataset** (think **hundreds of GBs**) stored as a CSV file in **cloud storage**.\n",
    "\n",
    "Each row represents a **user event** (such as a click, view, or purchase) along with:\n",
    "- when the event happened\n",
    "- which country it came from\n",
    "- which device was used\n",
    "- the transaction amount (if any)\n",
    "\n",
    "Because the dataset is **too large to fit into a single machine**, it must be processed using **Apache Spark on Databricks**.\n",
    "\n",
    "For learning purposes, we use a **smaller sample file** (`big_events_50k.csv`) that follows the **same structure and processing pattern** as the real large dataset.\n",
    "\n",
    "The input data already exists in **your Unity Catalog / database storage** and needs to be read, processed, and stored in an optimized format.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps using Spark:\n",
    "\n",
    "1. **Read** the large CSV dataset (`big_events_50k.csv`) from **your catalog / database storage**.  \n",
    "2. Let Spark **automatically distribute the data** across multiple executors.  \n",
    "3. **Aggregate** the data to calculate:\n",
    "   - total number of events\n",
    "   - total transaction amount  \n",
    "   grouped by **country** and **device**.\n",
    "4. **Write** the aggregated result in **Delta format** for efficient analytics.  \n",
    "5. **Validate** the output by reading it back and displaying sample records.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- The input CSV file is already available in **your Databricks catalog or database storage**.  \n",
    "- The dataset contains the following columns:  \n",
    "  `event_id`, `event_time`, `country`, `device`, `amount`\n",
    "- The file is large enough that **single-machine processing is not feasible**.  \n",
    "- Spark handles:\n",
    "  - file partitioning\n",
    "  - parallel execution\n",
    "  - fault tolerance automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- **Output Format:** Delta table  \n",
    "- **Output Location:**  \n",
    "  Stored in **your catalog / database** (Silver / curated layer)\n",
    "\n",
    "### **Expected Columns**\n",
    "\n",
    "| country | device | event_count | total_amount |\n",
    "|--------|--------|-------------|---------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- You do **not** need to manually split the file ‚Äî Spark does this for you.  \n",
    "- The **same Spark code works** for both small and very large datasets.  \n",
    "- Scalability comes from **distributed execution**, not from changing logic.  \n",
    "- Delta format helps with **reliability, performance, and analytics**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Example Output (simplified)\n",
    "\n",
    "| country | device  | event_count | total_amount |\n",
    "|--------|---------|-------------|---------------|\n",
    "| India  | Android | 12,450      | 8,945,230.50 |\n",
    "| USA    | iOS     | 9,820       | 7,112,980.75 |\n",
    "| UK     | Web     | 6,310       | 3,456,120.00 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b25755a-bece-4dad-bedf-ba56cf4c8e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read the dataset directly from cloud storage using Spark\n",
    "2. Let Spark automatically split the file into partitions\n",
    "3. Process partitions in parallel across executors\n",
    "4. Apply transformations using Spark DataFrame APIs\n",
    "5. Write results in an optimized format (Delta / Parquet)\n",
    "\n",
    "Spark handles:\n",
    "- Distributed processing\n",
    "- Lazy evaluation\n",
    "- Query optimization\n",
    "- Fault tolerance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9bb58d6-46e6-46ce-ac47-d3776d759cab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "input_path = \"your_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0c0656-a226-4ef3-a143-d3e361f85f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(input_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "660113fe-b0f3-4a00-95f7-d4861f9b963a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f593f9e-f00d-4411-95ea-2a467121c580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏èInput data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb797876-cb5f-430e-9f1c-3e03ce6bdd95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e1ad39-008b-4065-a22d-4146b729510c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üîç Spark does **not** load the entire dataset into one machine.\n",
    "\n",
    "- The file is split into partitions\n",
    "- Each partition is processed by a different executor\n",
    "- This same approach works for 50k rows or 500 GB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d910a6b-fde0-4bf4-a095-1e89dd8ca085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Transformation\n",
    "\n",
    "### Business Question:\n",
    "**What is the total number of events and total transaction amount\n",
    "per country and device?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dcdbfc1-3fe9-410e-9252-d82e3074bbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_df = (\n",
    "    df.groupBy(\"country\", \"device\")\n",
    "      .agg(\n",
    "          F.count(\"*\").alias(\"event_count\"),\n",
    "          F.sum(\"amount\").alias(\"total_amount\")\n",
    "      )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c58519c5-fd92-4783-80c4-392fa3ddc00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(agg_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def406b6-6f52-4724-88b5-2f0c719b2965",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üíæ Writing Output\n",
    "\n",
    "We write the results using **Delta Lake**, which is optimized for analytics\n",
    "and supported natively in Databricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbc9e939-67ab-45a3-9617-7f53dba1b398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg_df.write \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .format(\"delta\") \\\n",
    "  .save(\"your_directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06162e4b-3d4f-40fa-abaf-a4165a7af8fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_df = (\n",
    "    spark.read\n",
    "         .format(\"delta\")\n",
    "         .load(\"your_directory\")\n",
    ")\n",
    "\n",
    "display(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3b12ec-d650-4b1e-bd3f-d3bbeddaa301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Why This Solution Scales to 500 GB\n",
    "\n",
    "### Key Spark Features Used:\n",
    "- **Distributed File Reading** from ADLS / S3\n",
    "- **Automatic Partitioning**\n",
    "- **Parallel Execution across Executors**\n",
    "- **Lazy Evaluation**\n",
    "- **Catalyst Query Optimizer**\n",
    "- **Tungsten Execution Engine**\n",
    "- **Fault Tolerance**\n",
    "\n",
    "üí° The same code runs regardless of dataset size.\n",
    "Only the cluster size changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d78eed-e9a1-434e-b4f3-af9834eec722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Spark enables processing of datasets **larger than a single machine**\n",
    "- Databricks simplifies cluster management and optimization\n",
    "- Using columnar formats like **Delta** improves performance\n",
    "- This approach is production-ready and industry standard\n",
    "\n",
    "This notebook demonstrates a scalable Spark solution\n",
    "for large-scale data processing.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Big_events_50k_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
