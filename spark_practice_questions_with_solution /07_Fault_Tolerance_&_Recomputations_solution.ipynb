{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bab9a0e-58ea-4c2e-86a1-204a1d98651c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8835e52-408d-4f2d-8687-60d486dc57ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üõ°Ô∏è Fault Tolerance & Recomputation in Spark\n",
    "\n",
    "This notebook explains how Apache Spark can **recover from node failures**\n",
    "during long-running jobs **without restarting from scratch**.\n",
    "\n",
    "We focus on Spark‚Äôs **lineage-based fault tolerance model** and how it enables\n",
    "automatic recomputation of lost data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15111e85-5bf1-469a-ba5d-1f8d89d7017e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `sales_orders_large.csv`\n",
    "\n",
    "**Example Columns:**\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `region`\n",
    "- `customer_id`\n",
    "- `category`\n",
    "- `quantity`\n",
    "- `amount`\n",
    "\n",
    "> ‚ö†Ô∏è The exact size of the dataset is not critical for this scenario.  \n",
    "The goal is to build a **multi-step transformation pipeline** and understand\n",
    "how Spark can recompute parts of it if something goes wrong.\n",
    "\n",
    "The dataset is assumed to be available in **your catalog / database storage**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c98c45d-d761-43a7-b824-cdb99e1c609e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are running a **long Spark job** with multiple transformation steps.\n",
    "\n",
    "Midway through execution:\n",
    "- One executor (node) in the Spark cluster **fails**\n",
    "- Some partitions of intermediate data are **lost**\n",
    "\n",
    "Despite this failure:\n",
    "- The job **does not restart from the beginning**\n",
    "- Spark **re-executes only the lost work**\n",
    "- The job **still completes successfully**\n",
    "\n",
    "Your task is to explain:\n",
    "- **What Spark is doing internally**\n",
    "- **Why the job can recover**\n",
    "- **How lineage enables recomputation**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "1. Build a **multi-step transformation pipeline** using Spark.\n",
    "2. Observe how Spark tracks transformations as a **lineage DAG**.\n",
    "3. Explain what happens when an executor fails and loses partitions.\n",
    "4. Understand how Spark **recomputes only the missing partitions**.\n",
    "5. Learn when **caching or checkpointing** helps shorten recomputation paths.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Spark is running on a distributed cluster.\n",
    "- Executors may fail due to:\n",
    "  - hardware issues\n",
    "  - network problems\n",
    "  - resource pressure\n",
    "- Spark uses **lazy evaluation**.\n",
    "- Spark Serverless or classic clusters may be used.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- A clear explanation of Spark‚Äôs **fault tolerance mechanism**\n",
    "- A working example showing a **multi-step Spark pipeline**\n",
    "- Evidence of how Spark tracks transformations using **lineage**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Spark is **fault-tolerant by design**.\n",
    "- Spark does not store all intermediate data eagerly.\n",
    "- Instead, Spark stores **how to recompute the data**.\n",
    "- This design makes Spark resilient to executor failures.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2a9e15-4cfb-43b8-9b3e-96e1eb58fc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Spark represents all computations as a **Directed Acyclic Graph (DAG)**.\n",
    "2. Each RDD or DataFrame records its **lineage**:\n",
    "   - source data\n",
    "   - transformations applied\n",
    "3. When an executor fails, Spark identifies **which partitions were lost**.\n",
    "4. Spark **re-runs only the tasks needed** to rebuild those partitions.\n",
    "5. Other completed partitions remain untouched.\n",
    "6. Optional caching or checkpointing can **shorten the recomputation path**.\n",
    "\n",
    "Spark handles:\n",
    "- Task re-execution\n",
    "- Dependency tracking\n",
    "- Partition-level recovery\n",
    "- Automatic retry logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d678dd-ac2d-466b-be2f-e4b503d02cb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3d5554-c898-4942-add2-3196ad4cf199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "sales_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"your_data\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b5dd078-2929-4af4-8396-f2225997fd08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28921170-2056-43a9-a4b5-82f80053d500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üõ¢Ô∏è Input Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e42862-402f-4a4d-a651-65961386bd59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sales_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df23da47-ba70-4613-8958-5f5d3cc4a3d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîÑ Building a Multi-Step Transformation Pipeline\n",
    "\n",
    "We create a chain of transformations to simulate a long-running job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd09d4e-1142-41e4-b19f-e7323e8acb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "step1 = sales_df.filter(F.col(\"amount\") > 0)\n",
    "\n",
    "step2 = step1.withColumn(\n",
    "    \"amount_with_tax\",\n",
    "    F.col(\"amount\") * 1.18\n",
    ")\n",
    "\n",
    "step3 = step2.withColumn(\n",
    "    \"year\",\n",
    "    F.year(\"order_date\")\n",
    ")\n",
    "\n",
    "step4 = (\n",
    "    step3\n",
    "        .groupBy(\"year\", \"region\")\n",
    "        .agg(F.sum(\"amount_with_tax\").alias(\"total_sales\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17ea8ad-34d8-4c9f-b953-946e0855185f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Viewing the Lineage (Logical Plan)\n",
    "\n",
    "Spark tracks the **entire transformation chain**, not just the final result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf252f19-cad6-443a-9a45-fd06b88ab9a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "step4.explain(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5681c5c-be1e-4a4c-a6ab-cbb838b3c886",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### What You‚Äôll See\n",
    "\n",
    "- A logical and physical plan showing:\n",
    "  - CSV read\n",
    "  - filters\n",
    "  - projections\n",
    "  - aggregation\n",
    "- This plan represents the **lineage DAG**\n",
    "- Spark uses this DAG for both **execution and recovery**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4f5290-0627-4894-9f26-0eeb4c119cdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üí• What Happens When a Node Fails?\n",
    "\n",
    "If an executor fails:\n",
    "- All partitions stored on that executor are **lost**\n",
    "- Spark checks the lineage DAG\n",
    "- Spark re-executes **only the tasks needed** to rebuild those partitions\n",
    "- Other completed partitions are **not recomputed**\n",
    "\n",
    "This is possible because:\n",
    "- RDDs and DataFrames are **immutable**\n",
    "- Transformations are **deterministic**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3deea9c-467a-434c-9bca-4071b06f3b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚è±Ô∏è Checkpointing (Conceptual)\n",
    "\n",
    "In classic Spark clusters, **checkpointing** is used to:\n",
    "- Persist intermediate results to reliable storage\n",
    "- Truncate very long lineage chains\n",
    "- Reduce recomputation cost after failures\n",
    "\n",
    "On **Databricks Serverless compute**:\n",
    "- Direct access to `sparkContext` is not available\n",
    "- Checkpointing behavior is **platform-managed**\n",
    "- Users cannot manually configure checkpoint directories\n",
    "\n",
    "Even though we do not execute checkpointing here,\n",
    "the concept is important for understanding how Spark\n",
    "limits recomputation depth in long-running pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1d2554-a6b4-4545-8555-8e51d6a17450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚úÖ Summary\n",
    "\n",
    "- Spark achieves fault tolerance using **lineage-based recomputation**\n",
    "- Executors can fail without causing job failure\n",
    "- Only **lost partitions** are recomputed\n",
    "- Lineage + immutability make this safe and deterministic\n",
    "- Caching and checkpointing help optimize recovery for long pipelines\n",
    "\n",
    "This notebook demonstrates one of the **core design principles**\n",
    "that makes Spark suitable for large-scale, distributed data processing.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Fault_Tolerance_&_Recomputations_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
