{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c7f4a6f-d400-497b-ad3e-18ac141007dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a582ba36-693e-4ffd-bf48-f697c432fabe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üß¨ Serialization & Cross-Language UDFs\n",
    "\n",
    "This notebook demonstrates how **heavy Python UDF usage** can slow down Spark jobs\n",
    "and how to redesign the pipeline using:\n",
    "\n",
    "- Built-in Spark SQL functions\n",
    "- Vectorized (Pandas) UDFs with Arrow\n",
    "- Efficient serialization formats (Avro)\n",
    "\n",
    "The goal is to **improve performance** and **enable clean interoperability**\n",
    "with a downstream microservice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "151e5b8e-661b-4f28-a864-7d15ad997fa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `events_for_udf_50k.csv` \n",
    "\n",
    "### Example Columns\n",
    "- `user_id`\n",
    "- `country`\n",
    "- `segment`\n",
    "- `event_type`\n",
    "- `amount`\n",
    "\n",
    "This dataset simulates user activity events used to compute\n",
    "a **risk / engagement score**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bb03381-3647-4dfb-b88b-20f76e1fa822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "Your Spark job:\n",
    "- Processes a medium-sized events dataset\n",
    "- Uses **Python UDFs heavily**\n",
    "- Runs noticeably slower than equivalent Scala jobs\n",
    "\n",
    "Additionally:\n",
    "- The processed data must be sent to a **downstream microservice**\n",
    "- The microservice expects **Avro-encoded data**\n",
    "\n",
    "You want to:\n",
    "- Reduce Python‚ÄìJVM serialization overhead\n",
    "- Improve Spark execution performance\n",
    "- Use a standardized serialization format for interoperability\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Redesign the job to:\n",
    "\n",
    "1. Minimize Python UDF usage\n",
    "2. Prefer Spark SQL / built-in expressions\n",
    "3. Use vectorized UDFs only if necessary\n",
    "4. Keep data columnar inside Spark\n",
    "5. Serialize final output in **Avro**\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Dataset fits comfortably in Spark but has enough rows to show UDF overhead\n",
    "- Business logic computes a per-row engagement score\n",
    "- Microservice understands Avro schema\n",
    "- Cluster is shared and performance-sensitive\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Optimized engagement score computation\n",
    "- Reduced serialization overhead\n",
    "- Avro output for downstream systems\n",
    "\n",
    "### Expected Outcome\n",
    "\n",
    "| Area | Result |\n",
    "|----|----|\n",
    "Python UDF overhead | Reduced |\n",
    "Execution speed | Improved |\n",
    "Serialization | Standardized (Avro) |\n",
    "Interoperability | Simplified |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- **Python UDFs are slow** because each row must move between the JVM and Python.\n",
    "- Spark SQL / DataFrame expressions run **inside the JVM** and benefit from:\n",
    "  - whole-stage code generation\n",
    "  - vectorized execution\n",
    "- **Always try built-in functions first** before writing any UDF.\n",
    "- If custom logic is unavoidable:\n",
    "  - Prefer **Pandas (vectorized) UDFs** over normal Python UDFs.\n",
    "  - Pandas UDFs use **Apache Arrow**, which reduces serialization overhead.\n",
    "- Keep data in **columnar formats (Parquet / Delta)** while processing inside Spark.\n",
    "- **Serialize only once at system boundaries** (for example, when sending data to a microservice).\n",
    "- **Avro is ideal for cross-language systems** because:\n",
    "  - it enforces schema\n",
    "  - it is compact and fast\n",
    "  - it supports schema evolution\n",
    "- A common anti-pattern is:\n",
    "  - heavy Python UDFs + CSV/JSON everywhere\n",
    "- A common production pattern is:\n",
    "  - Spark SQL / Scala logic ‚Üí columnar storage ‚Üí Avro at the boundary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "920e7817-64dd-4f93-ac18-851e4147168f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Identify Python UDF bottlenecks\n",
    "2. Replace row-based UDFs with Spark SQL expressions\n",
    "3. Use Pandas UDFs only if custom Python logic is unavoidable\n",
    "4. Keep data in columnar formats inside Spark\n",
    "5. Serialize once at the boundary using Avro\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "15_Serialization_&_Cross_Language_UDFs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
