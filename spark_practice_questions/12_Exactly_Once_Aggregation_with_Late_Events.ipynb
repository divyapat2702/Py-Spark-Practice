{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc489d63-933f-4d89-a13a-cbe3165d78a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13a89e60-dda4-4839-8383-9551e07b0086",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üîÅ Exactly-Once Aggregation with Late Events\n",
    "\n",
    "This notebook demonstrates how to build a **near exactly-once**\n",
    "real-time aggregation pipeline using **Spark Structured Streaming**.\n",
    "\n",
    "The pipeline handles:\n",
    "- late-arriving events (up to 2 hours)\n",
    "- streaming replays after failures\n",
    "- prevention of double counting using idempotent writes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1135d3cb-3383-49c0-be03-fb4066c927ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `transactions_stream_large.csv`  \n",
    "\n",
    "### Columns:\n",
    "- `event_time`\n",
    "- `processing_time`\n",
    "- `txn_id`\n",
    "- `customer_id`\n",
    "- `amount`\n",
    "- `status`\n",
    "\n",
    "> ‚ö†Ô∏è In production, this data would arrive from **Kafka**.  \n",
    "> For learning purposes, we simulate streaming using files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab12c9ed-7dd1-4583-8d0a-5a4bed87933d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are building a **real-time transaction monitoring system**.\n",
    "\n",
    "Challenges:\n",
    "- Events can arrive **late (up to 2 hours)**\n",
    "- Failures may cause Spark to **replay micro-batches**\n",
    "- You must **avoid double counting**\n",
    "\n",
    "The business requires:\n",
    "- accurate aggregates\n",
    "- tolerance to late data\n",
    "- fault-tolerant recovery without restarting the job\n",
    "\n",
    "Your task is to design a streaming job that provides\n",
    "**near exactly-once aggregates**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "1. Read transaction events as a stream\n",
    "2. Use **event-time processing**\n",
    "3. Allow late data (up to 2 hours) using watermarks\n",
    "4. Aggregate transactions per hour and per customer\n",
    "5. Write results in a way that **prevents double counting**\n",
    "6. Ensure the job can safely recover from failures\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Events may arrive **out of order**\n",
    "- Late events can arrive up to **2 hours**\n",
    "- Spark Serverless compute is used\n",
    "- Unity Catalog storage is available\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Hourly aggregated transaction amounts per customer\n",
    "- Aggregates must be:\n",
    "  - fault-tolerant\n",
    "  - idempotent\n",
    "  - safe against reprocessing\n",
    "\n",
    "### Expected Columns\n",
    "\n",
    "| window.start | window.end | customer_id | total_amount |\n",
    "|--------------|------------|-------------|--------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Spark provides **exactly-once processing of source offsets**\n",
    "- Aggregations are **near exactly-once**\n",
    "- Idempotent sinks prevent double counting\n",
    "- Watermarks define how late data is handled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0714b192-1d74-4648-880f-fd19edbd61d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read transactions as a streaming DataFrame\n",
    "2. Parse timestamps and filter valid events\n",
    "3. Use **event-time watermarks** (2 hours)\n",
    "4. Perform windowed aggregations\n",
    "5. Write aggregates to a **transactional staging table**\n",
    "6. Use **MERGE-based upserts** into the final table\n",
    "\n",
    "Spark handles:\n",
    "- offset tracking\n",
    "- state management\n",
    "- fault recovery via checkpoints\n",
    "\n",
    "Exactly-once behavior is achieved by combining:\n",
    "- checkpointed offsets\n",
    "- deterministic aggregation keys\n",
    "- idempotent MERGE writes\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "12_Exactly_Once_Aggregation_with_Late_Events",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
