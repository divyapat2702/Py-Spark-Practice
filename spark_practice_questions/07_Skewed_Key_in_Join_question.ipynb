{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cbe260-efa7-44ad-8e9d-e811e3aead9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1682385c-e2f2-4ee2-9296-3e4a1b9f47a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚öñÔ∏è Handling Skewed Keys in Spark Joins\n",
    "\n",
    "This notebook demonstrates how **data skew** can severely impact Spark join performance\n",
    "and how to **detect and handle skewed keys** using proven Spark techniques.\n",
    "\n",
    "We focus on a common real-world issue where **a few keys dominate the data distribution**\n",
    "and cause one or more tasks to run significantly slower than others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b5a06ba-99d2-4741-aec3-ed63b7868326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "### Dataset A (Large & Skewed)\n",
    "**Dataset Name:** `transactions_a_skewed_large.csv`\n",
    "\n",
    "### Dataset B (Large & Skewed)\n",
    "**Dataset Name:** `transactions_b_skewed_large.csv`\n",
    "\n",
    "> ‚ö†Ô∏è These datasets simulate a real-world scenario where  \n",
    "a small number of `customer_id`s have **millions of records**,  \n",
    "while most customers have only a few.\n",
    "\n",
    "Both datasets are assumed to be available in **your catalog / database storage**.\n",
    "\n",
    "### Example Columns:\n",
    "- `transaction_id`\n",
    "- `customer_id`\n",
    "- `transaction_date`\n",
    "- `amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd5d4e5-9cbc-423e-a0ed-c57d4fbebc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are joining **two large transactional DataFrames** on `customer_id`.\n",
    "\n",
    "During execution, you notice that:\n",
    "- One or two tasks take **much longer** than others\n",
    "- Most tasks finish quickly, but a few keep running\n",
    "- Overall job time is dominated by a **single slow task**\n",
    "\n",
    "This usually indicates **data skew**, where a small number of keys\n",
    "(e.g., certain customers) have a **disproportionately large number of records**.\n",
    "\n",
    "Your goal is to:\n",
    "- **Detect** the skew\n",
    "- **Understand** why it happens\n",
    "- **Fix** the skew so the join runs efficiently\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps using Spark:\n",
    "\n",
    "1. **Read** both skewed transaction datasets.\n",
    "2. **Detect skewed keys** by analyzing record counts per `customer_id`.\n",
    "3. Confirm skew symptoms using **Spark UI**.\n",
    "4. **Handle skew** using key salting.\n",
    "5. (Optional) Enable **Adaptive Query Execution (AQE)** for automatic skew handling.\n",
    "6. Perform the join efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Both datasets are large and distributed across the cluster.\n",
    "- A small number of `customer_id`s are extremely frequent (hot keys).\n",
    "- Spark join performance is impacted by uneven partition sizes.\n",
    "- Spark Serverless or classic clusters may be used.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- **Joined DataFrame** with balanced execution\n",
    "- Reduced task skew and improved join performance\n",
    "\n",
    "### **Join Key**\n",
    "- `customer_id`\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes \n",
    "\n",
    "- Spark distributes work **by key** during joins.\n",
    "- If one key has far more records, **one task gets overloaded**.\n",
    "- Skew causes:\n",
    "  - slow tasks\n",
    "  - poor CPU utilization\n",
    "  - long job runtimes\n",
    "- Detecting skew early is critical for scalable pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a34b96-0871-4c38-a88c-fb58cf7e601b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read both large transaction datasets into Spark DataFrames.\n",
    "2. Detect skew by grouping on `customer_id` and identifying unusually high counts.\n",
    "3. Validate skew by observing slow or heavy tasks in the Spark UI.\n",
    "4. Identify **hot keys** (customers with extremely high record counts).\n",
    "5. Apply **salting** to the hot keys to spread their records across multiple partitions.\n",
    "6. Join the salted DataFrames on both `customer_id` and `salt`.\n",
    "7. Optionally enable **Adaptive Query Execution (AQE)** to let Spark handle skew automatically.\n",
    "\n",
    "Spark handles:\n",
    "- Distributed join execution\n",
    "- Task scheduling and partitioning\n",
    "- Optimizations via AQE when enabled\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Skewed_Key_in_Join_question",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
