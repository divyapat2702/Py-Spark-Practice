{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01a08ce1-efa4-4f9c-8335-92dd4f73de17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0f1d3b-d710-4b46-84e8-6976953c9561",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ‚è±Ô∏è Sliding Window Metrics with Structured Streaming\n",
    "\n",
    "This notebook demonstrates how to compute a **rolling ‚Äúlast 30 minutes error count‚Äù**\n",
    "from application logs using **Spark Structured Streaming**.\n",
    "\n",
    "We simulate real-time ingestion by converting an existing large CSV file\n",
    "into multiple small files and reading them as a **stream**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2747bc09-c748-408e-bde9-446d309c0613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `app_logs_large.csv`  \n",
    "\n",
    "\n",
    "### Columns:\n",
    "- `event_time`\n",
    "- `level`\n",
    "- `service`\n",
    "- `message`\n",
    "\n",
    "> ‚ö†Ô∏è In real production systems, logs arrive continuously via **Kafka**.  \n",
    "> For learning purposes, we simulate streaming using files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0361dff6-75fa-4d2f-9f4f-f432455e21e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "The business wants a **real-time operational metric**:\n",
    "\n",
    "> **‚ÄúHow many ERROR logs occurred in the last 30 minutes?‚Äù**\n",
    "\n",
    "Requirements:\n",
    "- Metric should update continuously\n",
    "- Must be based on **event time**\n",
    "- Must handle **late-arriving events**\n",
    "- Should scale as log volume grows\n",
    "\n",
    "You are asked to implement this using **Spark Structured Streaming**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "1. Read historical log data as batch\n",
    "2. Convert it into a streaming-friendly format\n",
    "3. Read logs as a streaming DataFrame\n",
    "4. Filter ERROR logs\n",
    "5. Apply a **30-minute sliding window** (slide every 1 minute)\n",
    "6. Add a watermark for late data\n",
    "7. Output rolling error counts\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Logs are event-time based\n",
    "- Data may arrive late\n",
    "- Databricks **Serverless compute** is used\n",
    "- Unity Catalog storage is available\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Streaming aggregation computing rolling error counts\n",
    "- Output grouped by:\n",
    "  - window\n",
    "  - service\n",
    "\n",
    "### Expected Columns\n",
    "\n",
    "| window.start | window.end | service | error_count |\n",
    "|--------------|------------|---------|-------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "- Streaming reads **directories**, not single files\n",
    "- New files represent new streaming data\n",
    "- Watermarks control late data handling\n",
    "- Sliding windows enable rolling metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae1faa74-c9e2-4c7d-95ca-8e5cad2b4fe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read existing logs as a batch DataFrame\n",
    "2. Split batch data into multiple small files\n",
    "3. Read those files using `readStream`\n",
    "4. Filter ERROR events\n",
    "5. Apply sliding window aggregation\n",
    "6. Write streaming results using a serverless-safe trigger\n",
    "\n",
    "Spark handles:\n",
    "- incremental processing\n",
    "- stateful window aggregation\n",
    "- fault tolerance using checkpoints\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10_Sliding_Window_for_Metrics_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
