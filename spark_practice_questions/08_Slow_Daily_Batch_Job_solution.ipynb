{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a93e3e6-2b12-48ae-a22a-10ff03fb21c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76d2256c-46e1-40a7-b4a1-33822747ac3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ¢ Optimizing a Slow Daily Spark Batch Job\n",
    "\n",
    "This notebook demonstrates how to **diagnose and optimize a slow-running daily Spark batch job**\n",
    "using a combination of **Spark UI analysis** and **code-level optimizations**.\n",
    "\n",
    "The job aggregates **daily sales by region** and has gradually slowed down\n",
    "as data volume increased.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6720c2b-d85b-489b-81cb-dd877bd1fbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“‚ Dataset\n",
    "\n",
    "**Dataset Name:** `sales_orders_large.csv`  \n",
    "**Description:** 30 days of historical sales data\n",
    "\n",
    "**Example Columns:**\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `region`\n",
    "- `customer_id`\n",
    "- `category`\n",
    "- `quantity`\n",
    "- `amount`\n",
    "\n",
    "The dataset is assumed to be available in **your catalog / database storage**.\n",
    "\n",
    "> âš ï¸ In real production systems, this dataset typically grows every day,\n",
    "which can cause batch jobs to slow down if not designed correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3447cc0d-1646-436e-ab70-8b5f21e908d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ—‚ï¸ Scenario\n",
    "\n",
    "You own a **daily Spark batch job** that aggregates total sales by region.\n",
    "\n",
    "Initially:\n",
    "- Runtime â‰ˆ **15 minutes**\n",
    "\n",
    "Over time:\n",
    "- Runtime slowly increased to **2 hours**\n",
    "\n",
    "No single change caused the slowdown â€” instead, it happened gradually as:\n",
    "- Data volume increased\n",
    "- More historical data accumulated\n",
    "- Inefficient read patterns became more expensive\n",
    "- Shuffle-heavy operations started processing much more data\n",
    "\n",
    "This is a **very common real-world problem** in batch data pipelines.\n",
    "\n",
    "Your goal is to:\n",
    "- **Diagnose** the performance bottlenecks using Spark UI\n",
    "- **Optimize** the job so it continues to scale as data grows\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Task\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "1. Use **Spark UI** to identify where the job is slow.\n",
    "2. Determine whether the job is:\n",
    "   - reading too much data\n",
    "   - performing large or unnecessary shuffles\n",
    "   - affected by data skew\n",
    "3. Apply **code-level optimizations** to:\n",
    "   - reduce the amount of data scanned\n",
    "   - reduce shuffle cost during aggregation\n",
    "   - use efficient storage formats\n",
    "4. Validate that the optimized job processes **only the required dayâ€™s data**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Assumptions\n",
    "\n",
    "- The job runs **once per day** for a single `process_date`.\n",
    "- Historical sales data is stored in **cloud object storage**.\n",
    "- Data volume **grows continuously** as new days are added.\n",
    "- Spark Serverless or classic Spark clusters may be used.\n",
    "- No changes were made to business logic â€” only data volume increased.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“¦ Deliverables\n",
    "\n",
    "- An **optimized daily aggregation job** that scales with growing data.\n",
    "- Aggregated output containing **total sales by region for a single day**.\n",
    "- A clear demonstration of **performance improvement** compared to the naive approach.\n",
    "\n",
    "### **Expected Output Columns**\n",
    "\n",
    "| order_date | region | total_sales_amount |\n",
    "|-----------|--------|--------------------|\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Notes \n",
    "\n",
    "- Performance problems usually appear **gradually** as data grows, not suddenly.\n",
    "- Always start optimization by **observing Spark UI**, not changing code blindly.\n",
    "- The biggest performance wins usually come from **reading less data**, not adding more compute.\n",
    "- Using **Parquet or Delta** enables:\n",
    "  - column pruning\n",
    "  - predicate pushdown\n",
    "- Partitioning data by frequently filtered columns (such as `order_date`) enables **partition pruning**.\n",
    "- Code that works well for small datasets may **fail to scale** without these optimizations.\n",
    "\n",
    "> ðŸ’¡ Tip:  \n",
    "> If a batch job becomes slower over time, assume a **data growth and data layout problem first**, not a Spark bug.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb71877a-8d91-444b-96b7-115c1eaee804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ§  Solution Strategy (High-Level)\n",
    "\n",
    "1. Use **Spark UI** to identify slow stages and expensive operations.\n",
    "2. Confirm whether the job is scanning **more data than necessary**.\n",
    "3. Convert raw CSV data into an **efficient columnar format** (Parquet or Delta).\n",
    "4. Ensure the dataset is **partitioned by order_date**.\n",
    "5. Modify the job to:\n",
    "   - read only the required date partition\n",
    "   - select only necessary columns\n",
    "6. Reduce shuffle overhead during aggregation.\n",
    "7. Write optimized output for downstream consumption.\n",
    "\n",
    "Spark handles:\n",
    "- Distributed execution\n",
    "- Partition pruning\n",
    "- Column pruning\n",
    "- Shuffle-based aggregations\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "08_Slow_Daily_Batch_Job_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
