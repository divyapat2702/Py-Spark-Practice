{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdc5bb57-f303-4189-bab0-47d6f313743a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e51851-39b3-4e96-a2c5-89970bdbaf52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üì¶ Big Data, Small Machines\n",
    "\n",
    "This notebook demonstrates how Apache Spark on Databricks can process\n",
    "datasets that are **too large to fit into the memory of a single machine**.\n",
    "\n",
    "We simulate a **500 GB CSV file** scenario using a smaller dataset,\n",
    "while following the **exact same processing pattern** used for large-scale data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68b53e3e-dc2e-405d-9b08-cb7dd73741e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `big_events_50k.csv`  \n",
    "**Storage Location:** Databricks File System (DBFS) (Refer Create a Databricks Catalog and Upload a CSV.pdf)\n",
    "\n",
    "> ‚ö†Ô∏è In real-world scenarios, this dataset would be a **500 GB CSV**\n",
    "stored in **ADLS / S3**.  \n",
    "Since we cannot upload such a large file here, we use **50k rows to simulate the same pattern**.\n",
    "\n",
    "### Example Columns:\n",
    "- `event_id`\n",
    "- `event_time`\n",
    "- `country`\n",
    "- `device`\n",
    "- `amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be20d76f-05cc-4085-aa6d-4edb8d72722f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are working with a **very large events dataset** (think **hundreds of GBs**) stored as a CSV file in **cloud storage**.\n",
    "\n",
    "Each row represents a **user event** (such as a click, view, or purchase) along with:\n",
    "- when the event happened\n",
    "- which country it came from\n",
    "- which device was used\n",
    "- the transaction amount (if any)\n",
    "\n",
    "Because the dataset is **too large to fit into a single machine**, it must be processed using **Apache Spark on Databricks**.\n",
    "\n",
    "For learning purposes, we use a **smaller sample file** (`big_events_50k.csv`) that follows the **same structure and processing pattern** as the real large dataset.\n",
    "\n",
    "The input data already exists in **your Unity Catalog / database storage** and needs to be read, processed, and stored in an optimized format.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps using Spark:\n",
    "\n",
    "1. **Read** the large CSV dataset (`big_events_50k.csv`) from **your catalog / database storage**.  \n",
    "2. Let Spark **automatically distribute the data** across multiple executors.  \n",
    "3. **Aggregate** the data to calculate:\n",
    "   - total number of events\n",
    "   - total transaction amount  \n",
    "   grouped by **country** and **device**.\n",
    "4. **Write** the aggregated result in **Delta format** for efficient analytics.  \n",
    "5. **Validate** the output by reading it back and displaying sample records.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- The input CSV file is already available in **your Databricks catalog or database storage**.  \n",
    "- The dataset contains the following columns:  \n",
    "  `event_id`, `event_time`, `country`, `device`, `amount`\n",
    "- The file is large enough that **single-machine processing is not feasible**.  \n",
    "- Spark handles:\n",
    "  - file partitioning\n",
    "  - parallel execution\n",
    "  - fault tolerance automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- **Output Format:** Delta table  \n",
    "- **Output Location:**  \n",
    "  Stored in **your catalog / database** (Silver / curated layer)\n",
    "\n",
    "### **Expected Columns**\n",
    "\n",
    "| country | device | event_count | total_amount |\n",
    "|--------|--------|-------------|---------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- You do **not** need to manually split the file ‚Äî Spark does this for you.  \n",
    "- The **same Spark code works** for both small and very large datasets.  \n",
    "- Scalability comes from **distributed execution**, not from changing logic.  \n",
    "- Delta format helps with **reliability, performance, and analytics**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Example Output (simplified)\n",
    "\n",
    "| country | device  | event_count | total_amount |\n",
    "|--------|---------|-------------|---------------|\n",
    "| India  | Android | 12,450      | 8,945,230.50 |\n",
    "| USA    | iOS     | 9,820       | 7,112,980.75 |\n",
    "| UK     | Web     | 6,310       | 3,456,120.00 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b25755a-bece-4dad-bedf-ba56cf4c8e2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Read the dataset directly from cloud storage using Spark\n",
    "2. Let Spark automatically split the file into partitions\n",
    "3. Process partitions in parallel across executors\n",
    "4. Apply transformations using Spark DataFrame APIs\n",
    "5. Write results in an optimized format (Delta / Parquet)\n",
    "\n",
    "Spark handles:\n",
    "- Distributed processing\n",
    "- Lazy evaluation\n",
    "- Query optimization\n",
    "- Fault tolerance\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_Big_events_50k_solution_question",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
