{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bab9a0e-58ea-4c2e-86a1-204a1d98651c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8835e52-408d-4f2d-8687-60d486dc57ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üõ°Ô∏è Fault Tolerance & Recomputation in Spark\n",
    "\n",
    "This notebook explains how Apache Spark can **recover from node failures**\n",
    "during long-running jobs **without restarting from scratch**.\n",
    "\n",
    "We focus on Spark‚Äôs **lineage-based fault tolerance model** and how it enables\n",
    "automatic recomputation of lost data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15111e85-5bf1-469a-ba5d-1f8d89d7017e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Dataset Name:** `sales_orders_large.csv`\n",
    "\n",
    "**Example Columns:**\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `region`\n",
    "- `customer_id`\n",
    "- `category`\n",
    "- `quantity`\n",
    "- `amount`\n",
    "\n",
    "> ‚ö†Ô∏è The exact size of the dataset is not critical for this scenario.  \n",
    "The goal is to build a **multi-step transformation pipeline** and understand\n",
    "how Spark can recompute parts of it if something goes wrong.\n",
    "\n",
    "The dataset is assumed to be available in **your catalog / database storage**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c98c45d-d761-43a7-b824-cdb99e1c609e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are running a **long Spark job** with multiple transformation steps.\n",
    "\n",
    "Midway through execution:\n",
    "- One executor (node) in the Spark cluster **fails**\n",
    "- Some partitions of intermediate data are **lost**\n",
    "\n",
    "Despite this failure:\n",
    "- The job **does not restart from the beginning**\n",
    "- Spark **re-executes only the lost work**\n",
    "- The job **still completes successfully**\n",
    "\n",
    "Your task is to explain:\n",
    "- **What Spark is doing internally**\n",
    "- **Why the job can recover**\n",
    "- **How lineage enables recomputation**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps:\n",
    "\n",
    "1. Build a **multi-step transformation pipeline** using Spark.\n",
    "2. Observe how Spark tracks transformations as a **lineage DAG**.\n",
    "3. Explain what happens when an executor fails and loses partitions.\n",
    "4. Understand how Spark **recomputes only the missing partitions**.\n",
    "5. Learn when **caching or checkpointing** helps shorten recomputation paths.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Spark is running on a distributed cluster.\n",
    "- Executors may fail due to:\n",
    "  - hardware issues\n",
    "  - network problems\n",
    "  - resource pressure\n",
    "- Spark uses **lazy evaluation**.\n",
    "- Spark Serverless or classic clusters may be used.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- A clear explanation of Spark‚Äôs **fault tolerance mechanism**\n",
    "- A working example showing a **multi-step Spark pipeline**\n",
    "- Evidence of how Spark tracks transformations using **lineage**\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Spark is **fault-tolerant by design**.\n",
    "- Spark does not store all intermediate data eagerly.\n",
    "- Instead, Spark stores **how to recompute the data**.\n",
    "- This design makes Spark resilient to executor failures.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2a9e15-4cfb-43b8-9b3e-96e1eb58fc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Spark represents all computations as a **Directed Acyclic Graph (DAG)**.\n",
    "2. Each RDD or DataFrame records its **lineage**:\n",
    "   - source data\n",
    "   - transformations applied\n",
    "3. When an executor fails, Spark identifies **which partitions were lost**.\n",
    "4. Spark **re-runs only the tasks needed** to rebuild those partitions.\n",
    "5. Other completed partitions remain untouched.\n",
    "6. Optional caching or checkpointing can **shorten the recomputation path**.\n",
    "\n",
    "Spark handles:\n",
    "- Task re-execution\n",
    "- Dependency tracking\n",
    "- Partition-level recovery\n",
    "- Automatic retry logic\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09_Fault_Tolerance_&_Recomputations_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
