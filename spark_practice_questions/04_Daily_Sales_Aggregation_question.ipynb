{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a87722d4-44d2-4280-aafa-acacfe42be44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f7fe4c8-99e9-4884-b134-59edae74e4dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üìÖ Daily Sales Aggregation (Batch)\n",
    "\n",
    "This notebook shows how to design a **daily Spark batch job**\n",
    "that processes **one day‚Äôs sales CSV file**, aggregates total sales\n",
    "by region, and writes the results to a curated table used by BI dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e0a5e8-fe03-4dfd-9d26-50dec3350c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "Daily sales files arrive as **individual CSVs**:\n",
    "\n",
    "- `sales_orders_2025-01-01.csv`\n",
    "- `sales_orders_2025-01-02.csv`\n",
    "- `sales_orders_2025-01-03.csv`\n",
    "- `sales_orders_2025-01-04.csv`\n",
    "- `sales_orders_2025-01-05.csv`\n",
    "\n",
    "**Location (example ‚Äì Databricks Volume)**\n",
    "\n",
    "\n",
    "### Schema\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `region`\n",
    "- `customer_id`\n",
    "- `amount`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6076ac5-2433-4001-b85e-c714a996f8ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "Your company receives **one sales CSV per day** in cloud storage\n",
    "(ADLS / Blob Storage or S3).\n",
    "\n",
    "Business requirements:\n",
    "- Process **only the new day‚Äôs file**\n",
    "- Compute **total sales amount per region**\n",
    "- Store results in a **curated table** for BI dashboards\n",
    "- Ensure the job is **safe to re-run** for the same day (idempotent)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Build a Spark batch job that:\n",
    "\n",
    "1. Accepts a `process_date` parameter\n",
    "2. Reads only the corresponding daily CSV file\n",
    "3. Aggregates total sales per region\n",
    "4. Writes results to a curated table\n",
    "5. Overwrites that day‚Äôs data if the job is re-run\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- Storage (ADLS / S3 / Volume) is already accessible in Spark\n",
    "- One CSV file exists per day\n",
    "- Job is scheduled daily via Airflow / ADF / cron\n",
    "- BI dashboards read from the curated table\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- Daily sales totals per region\n",
    "- Output stored in Delta / Parquet\n",
    "- Data partitioned by `order_date`\n",
    "\n",
    "### Expected Output Schema\n",
    "\n",
    "| order_date | region | total_sales_amount |\n",
    "|------------|--------|--------------------|\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Always **parameterize dates** in batch jobs\n",
    "- Never scan all files when only one day is required\n",
    "- File naming conventions (`YYYY-MM-DD`) are enough for daily jobs\n",
    "- Design jobs to be **idempotent** so re-runs don‚Äôt duplicate data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "432951e6-980e-47a8-bdc1-0a41386d5b2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. Receive `process_date` from scheduler\n",
    "2. Build input file path using the date\n",
    "3. Read only that file\n",
    "4. Aggregate sales by region\n",
    "5. Write output partitioned by `order_date`\n",
    "6. Overwrite the day‚Äôs partition for idempotency\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Daily_Sales_Aggregation_question",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
