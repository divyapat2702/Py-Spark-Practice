{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a8bf0d-fcce-4310-9718-cfc9f14e6242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<div style=\"display: flex; align-items: center; gap: 18px; margin-bottom: 15px;\">\n",
    "  <img src=\"https://files.codebasics.io/v3/images/sticky-logo.svg\" alt=\"Codebasics Logo\" style=\"display: inline-block;\" width=\"130\">\n",
    "  <h1 style=\"font-size: 34px; color: #1f4e79; margin: 0; display: inline-block;\">Codebasics Practice Room - Data Engineering Bootcamp </h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8f7c62-70fd-435a-8776-49317602a3ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### üîÅ Reusing Computation Efficiently in Spark\n",
    "\n",
    "This notebook demonstrates how Apache Spark can **avoid recomputing the same expensive transformations**\n",
    "when the **same cleaned DataFrame is used multiple times** within a single job.\n",
    "\n",
    "We focus on **performance optimization** using Spark‚Äôs **cache / persist** mechanism,\n",
    "which is critical when working with **large datasets**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36aa341-aa7a-49ce-b817-90b82752ad85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üìÇ Dataset\n",
    "\n",
    "**Primary Dataset:** `sales_orders_raw_with_issues.csv`  \n",
    "**Optional Large Dataset:** `sales_orders_large.csv`\n",
    "\n",
    "> ‚ö†Ô∏è In real-world scenarios, sales datasets can be **very large (GBs or more)**.  \n",
    "To keep this exercise practical, we assume the dataset already exists in  \n",
    "**your catalog / database storage**.\n",
    "\n",
    "### Example Columns:\n",
    "- `order_id`\n",
    "- `order_date`\n",
    "- `customer_id`\n",
    "- `region`\n",
    "- `category`\n",
    "- `amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e19cb669-0d7d-4214-aa68-20adec5ae542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üóÇÔ∏è Scenario\n",
    "\n",
    "You are working with a **sales orders dataset** that requires multiple cleaning steps\n",
    "before it can be used for reporting.\n",
    "\n",
    "After cleaning, the same cleaned DataFrame is used to generate **5 different reports**\n",
    "inside the **same Spark job**.\n",
    "\n",
    "Without optimization, Spark will **re-run the same cleaning transformations**\n",
    "every time a report is computed, leading to:\n",
    "- unnecessary recomputation\n",
    "- increased job runtime\n",
    "- wasted cluster resources\n",
    "\n",
    "Your goal is to **optimize the job** so that the cleaning logic runs **only once**.\n",
    "\n",
    "The input data already exists in **your catalog / database storage** and needs to be\n",
    "cleaned, reused, and analyzed efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Task\n",
    "\n",
    "Perform the following steps using Spark:\n",
    "\n",
    "1. **Read** the raw sales orders dataset.\n",
    "2. Apply all **cleaning and standard transformations** to create a cleaned DataFrame.\n",
    "3. **Cache or persist** the cleaned DataFrame so Spark materializes it once.\n",
    "4. Reuse the cleaned DataFrame to generate **multiple reports**.\n",
    "5. **Unpersist** the DataFrame after all reports are completed to free resources.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Assumptions\n",
    "\n",
    "- The raw dataset contains data quality issues (nulls, duplicates, invalid values).\n",
    "- The cleaned DataFrame is reused multiple times within the same Spark job.\n",
    "- Spark uses **lazy evaluation**, so transformations are not executed until an action occurs.\n",
    "- The dataset is large enough that recomputation would be expensive.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Deliverables\n",
    "\n",
    "- **Cleaned DataFrame:** reused across multiple reports  \n",
    "- **Reports:** Aggregations derived from the same cached DataFrame  \n",
    "\n",
    "### **Example Reports**\n",
    "- Total sales by region\n",
    "- Total sales by category\n",
    "- Daily sales trends\n",
    "- (Additional reports can be added without re-running cleaning logic)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Notes\n",
    "\n",
    "- Spark does **not** automatically remember intermediate results.\n",
    "- Without caching, Spark **recomputes transformations for every action**.\n",
    "- Caching is useful when:\n",
    "  - a DataFrame is expensive to compute\n",
    "  - the same DataFrame is reused multiple times\n",
    "- Always unpersist cached data when it is no longer needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51aa26f7-c116-46ed-b481-b96e8f0b3bed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üß† Solution Strategy (High-Level)\n",
    "\n",
    "1. **Read the raw sales orders dataset** from your catalog / database storage using Spark.\n",
    "2. Apply all **data cleaning and standard transformations** once to create a cleaned DataFrame (`clean_df`).\n",
    "3. **Cache or persist** the cleaned DataFrame so Spark stores the computed result after the first action.\n",
    "4. Trigger an **action** (such as `count()` or the first report) to materialize the cached DataFrame.\n",
    "5. Reuse the cached `clean_df` across **multiple reports** (aggregations, groupings, joins).\n",
    "6. After all reports are generated, **unpersist** the DataFrame to free up cluster memory.\n",
    "\n",
    "Spark handles:\n",
    "- Avoiding repeated recomputation of expensive transformations\n",
    "- Efficient reuse of intermediate results\n",
    "- Memory management for cached data\n",
    "- Parallel execution across executors\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "06_Repeated_Transformation_on_Same_Data_question",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
